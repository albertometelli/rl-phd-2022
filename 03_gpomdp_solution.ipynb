{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/albertometelli/rl-phd-2022/blob/main/03_gpomdp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AoxOjIlOImwx"
   },
   "source": [
    "# G(PO)MDP\n",
    "\n",
    "This notebook is inspired to the Stable Baselines3 tutorial available at [https://github.com/araffin/rl-tutorial-jnrr19](https://github.com/araffin/rl-tutorial-jnrr19).\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will implement a very basic version of **G(PO)MDP**.\n",
    "\n",
    "### Links\n",
    "\n",
    "Open AI Gym Github: [https://github.com/openai/gym](https://github.com/openai/gym)\n",
    "\n",
    "Open AI Gym Documentation: [https://www.gymlibrary.ml](https://www.gymlibrary.ml)\n",
    "\n",
    "Stable Baselines 3 Github:[https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)\n",
    "\n",
    "Stable Baseline 3 Documentation: [https://stable-baselines3.readthedocs.io/en/master/](https://stable-baselines3.readthedocs.io/en/master/)\n",
    "\n",
    "## Install Dependencies and Stable Baselines3 Using Pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sp8rSS4DIhEV"
   },
   "outputs": [],
   "source": [
    "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3\n",
    "stable_baselines3.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Recording\n",
    "\n",
    "In Google Colab it is not possible to render the Gym environments, so we need to record a video and then reproduce it. Here are the helper functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up fake display; otherwise rendering will fail\n",
    "import os\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from IPython import display as ipythondisplay\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
    "os.environ['DISPLAY'] = ':1'\n",
    "\n",
    "def show_videos(video_path='', prefix=''):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/eleurent/highway-env\n",
    "\n",
    "    :param video_path: (str) Path to the folder containing videos\n",
    "    :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
    "    \"\"\"\n",
    "    html = []\n",
    "    for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "        html.append('''<video alt=\"{}\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                    </video>'''.format(mp4, video_b64.decode('ascii')))\n",
    "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
    "\n",
    "\n",
    "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
    "    \"\"\"\n",
    "    :param env_id: (str)\n",
    "    :param model: (RL model)\n",
    "    :param video_length: (int)\n",
    "    :param prefix: (str)\n",
    "    :param video_folder: (str)\n",
    "    \"\"\"\n",
    "    eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
    "    # Start the video at step=0 and record 500 steps\n",
    "    eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
    "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
    "                              name_prefix=prefix)\n",
    "\n",
    "    obs = eval_env.reset()\n",
    "    for _ in range(video_length):\n",
    "        action, _ = model.predict(obs[0])\n",
    "        obs, _, _, _ = eval_env.step([action])\n",
    "\n",
    "    # Close the video recorder\n",
    "    eval_env.close()\n",
    "\n",
    "    \n",
    "def render(env_id, policy, video_length=500, prefix='', video_folder='videos/'):\n",
    "    record_video(env_id, policy, video_length, prefix, video_folder)\n",
    "    show_videos(video_folder, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "A helper function to evaluate policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, policy, gamma=1., num_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param env: (Env object) the Gym environment\n",
    "    :param policy: (BasePolicy object) the policy in stable_baselines3\n",
    "    :param gamma: (float) the discount factor\n",
    "    :param num_episodes: (int) number of episodes to evaluate it\n",
    "    :return: (float) Mean reward for the last num_episodes\n",
    "    \"\"\"\n",
    "    discounter = 1.\n",
    "    all_episode_rewards = []\n",
    "    for i in range(num_episodes): # iterate over the episodes\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done: # iterate over the steps until termination\n",
    "            action, _ = policy.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards.append(reward * discounter) # compute discounted reward\n",
    "            discounter *= gamma\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    std_episode_reward = np.std(all_episode_rewards) / np.sqrt(num_episodes - 1)\n",
    "    print(\"Mean reward:\", mean_episode_reward, \n",
    "          \"Std reward:\", std_episode_reward,\n",
    "          \"Num episodes:\", num_episodes)\n",
    "\n",
    "    return mean_episode_reward, std_episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "A helper function to plot the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_results(results):\n",
    "    plt.figure()\n",
    "    \n",
    "    _mean = []\n",
    "    _std = []\n",
    "    for m, s in results:\n",
    "        _mean.append(m)\n",
    "        _std.append(s)\n",
    "        \n",
    "    _mean = np.array(_mean)\n",
    "    _std = np.array(_std)\n",
    "        \n",
    "    ts = np.arange(len(_mean))\n",
    "    plt.plot(ts, _mean, label='G(PO)MDP')\n",
    "    plt.fill_between(ts, _mean-_std, _mean+_std, alpha=.2)\n",
    "        \n",
    "    plt.xlabel('Trajectories')\n",
    "    plt.ylabel('Average return')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G(PO)MDP\n",
    "\n",
    "![ss](gpomdp.png)\n",
    "\n",
    "**References**\n",
    "\n",
    "Baxter, Jonathan, and Peter L. Bartlett. \"Infinite-horizon policy-gradient estimation.\" Journal of Artificial Intelligence Research 15 (2001): 319-350."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "\n",
    "We will use a Gaussian policy, linear in the state variables and with fixed (non-learnable) standard deviation. \n",
    "\n",
    "$$\n",
    "\\pi_{\\boldsymbol{\\theta}}(a|\\mathbf{s}) = \\mathcal{N}(a| \\boldsymbol{\\theta}^T \\mathbf{s}, \\sigma^2)\n",
    "$$\n",
    "\n",
    "The policy must implement the usual `predict` method and some additional methods for computing the policy gradient. Specifically, we will need a `grad_log` method to return the gradient of the logarithm of the policy (the score):\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol{\\theta}} \\log \\pi_{\\boldsymbol{\\theta}}(a|\\mathbf{s})= \\frac{(a - \\boldsymbol{\\theta}^T \\mathbf{s})\\mathbf{s}}{\\sigma^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Complete the implementation of the methods `predict` and `grad_log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPolicy:\n",
    "    \n",
    "    def __init__(self, dim, std=0.1):\n",
    "        \"\"\"\n",
    "        :param dim: number of state variables\n",
    "        :param std: fixed standard deviation\n",
    "        \"\"\"\n",
    "        \n",
    "        self.std = std\n",
    "        self.dim = dim\n",
    "        self.theta = np.zeros((dim,))  # zero initializatoin\n",
    "    \n",
    "    def get_theta(self):\n",
    "        return self.theta\n",
    "    \n",
    "    def set_theta(self, value):\n",
    "        self.theta = value\n",
    "    \n",
    "    def predict(self, obs):\n",
    "        \"\"\"\n",
    "        :param obs: (ndarray) the state observation (dim,)\n",
    "        :return: the sampled action and the same observation\n",
    "        \"\"\"\n",
    "        action = 0.\n",
    "        \n",
    "        #TODO\n",
    "        \n",
    "        return np.array([action]), obs\n",
    "    \n",
    "    def grad_log(self, obs, action):\n",
    "        \"\"\"\n",
    "        :param obs: (ndarray) the state observation (dim,)\n",
    "        :param action: (float) the action\n",
    "        :return: (ndarray) the score of the policy (dim,)\n",
    "        \"\"\"\n",
    "        grad_log = 0.\n",
    "        \n",
    "        #TODO\n",
    "        \n",
    "        return grad_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPolicy:\n",
    "    \n",
    "    def __init__(self, dim, std=0.1):\n",
    "        \"\"\"\n",
    "        :param dim: number of state variables\n",
    "        :param std: fixed standard deviation\n",
    "        \"\"\"\n",
    "        \n",
    "        self.std = std\n",
    "        self.dim = dim\n",
    "        self.theta = np.zeros((dim,))  # zero initializatoin\n",
    "    \n",
    "    def get_theta(self):\n",
    "        return self.theta\n",
    "    \n",
    "    def set_theta(self, value):\n",
    "        self.theta = value\n",
    "    \n",
    "    def predict(self, obs):\n",
    "        mean = np.dot(obs, self.theta)\n",
    "        action = mean + np.random.randn() * self.std\n",
    "        return np.array([action]), obs\n",
    "    \n",
    "    def grad_log(self, obs, action):\n",
    "        mean = np.dot(obs, self.theta)\n",
    "        grad_log = (action - mean) * obs / self.std ** 2\n",
    "        return grad_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine\n",
    "\n",
    "We provide the already implemented skeleton of the training routine that samples at every iterations $m$ trajectories from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_rollouts(env, policy, m, T):\n",
    "    \"\"\"\n",
    "    Collects m rollouts by running the policy in the\n",
    "        environment\n",
    "    :param env: (Env object) the Gym environment\n",
    "    :param policy: (Policy object) the policy\n",
    "    :param gamma: (float) the discount factor\n",
    "    :param m: (int) number of episodes per iterations\n",
    "    :param K: (int) maximum number of iterations\n",
    "    :param theta0: (ndarray) initial parameters (d,)\n",
    "    :param alpha: (float) the constant learning rate\n",
    "    :param T: (int) the trajectory horizon\n",
    "    :return: (list of lists) one list per episode\n",
    "                each containing triples (s, a, r)\n",
    "    \"\"\"\n",
    "    \n",
    "    ll = []\n",
    "    for j in range(m):\n",
    "        s = env.reset()\n",
    "        t = 0\n",
    "        done = False\n",
    "        l = []\n",
    "        while t < T and not done:\n",
    "            a, _ = policy.predict(s)\n",
    "            s1, r, done, _ = env.step(a)\n",
    "            l.append((s, a, r))\n",
    "            s = s1\n",
    "            t += 1\n",
    "        ll.append(l)\n",
    "    return ll\n",
    "            \n",
    "def train(env, policy, gamma, m, K, alpha, T):\n",
    "    \"\"\"\n",
    "    Train a policy with G(PO)MDP\n",
    "    :param env: (Env object) the Gym environment\n",
    "    :param policy: (Policy object) the policy\n",
    "    :param gamma: (float) the discount factor\n",
    "    :param m: (int) number of episodes per iterations\n",
    "    :param K: (int) maximum number of iterations\n",
    "    :param alpha: (float) the constant learning rate\n",
    "    :param T: (int) the trajectory horizon\n",
    "    :return: list (ndarray, ndarray) the evaluations\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Evaluate the initial policy\n",
    "    res = evaluate(env, policy, gamma)\n",
    "    results.append(res)\n",
    "    \n",
    "    for k in range(K):\n",
    "        \n",
    "        print('Iteration:', k)\n",
    "        \n",
    "        # Generate rollouts\n",
    "        rollouts = collect_rollouts(env, policy, m, T)\n",
    "        \n",
    "        # Get policy parameter\n",
    "        theta = policy.get_theta()\n",
    "        \n",
    "        # Call your G(PO)MDP estimator\n",
    "        pg = gpomdp(rollouts, policy, gamma)\n",
    "        \n",
    "        # Update policy parameter\n",
    "        theta = theta + alpha * pg\n",
    "        \n",
    "        # Set policy parameters\n",
    "        policy.set_theta(theta)\n",
    "        \n",
    "        # Evaluate the updated policy\n",
    "        res = evaluate(env, policy, gamma)\n",
    "        results.append(res)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Complete the following function `gpomdp` that computes the G(PO)MDP gradient estimator given rollout trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpomdp(rollouts, policy, gamma):\n",
    "    \"\"\"\n",
    "    :param rollouts: (list of lists) generated by 'collect_rollouts'\n",
    "    :param policy: (Policy object) the policy\n",
    "    :param gamma: (float) the discount factor\n",
    "    :return: (ndarray) the policy gradient (dim,)\n",
    "    \"\"\"\n",
    "    \n",
    "    grad = 0\n",
    "    \n",
    "    #TODO\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a possible implementation of G(PO)MDP that turns out to be very inefficient! It can be improved by precomputing the scores at the beginning and using matrix opertions instead of for cicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpomdp_inefficient(rollouts, policy, gamma):\n",
    "    \n",
    "    grad = 0\n",
    "    \n",
    "    # Very very inefficient implementation!\n",
    "    for roll in rollouts:\n",
    "        H = len(roll)\n",
    "        \n",
    "        sum_rew = 0.\n",
    "        for t in range(H):\n",
    "            \n",
    "            sum_scores = 0.\n",
    "            for l in range(t + 1):\n",
    "                s, a, _ = roll[l]\n",
    "                score = policy.grad_log(s, a)\n",
    "                sum_scores += score\n",
    "            \n",
    "            _, _, r = roll[t]\n",
    "            sum_rew += gamma ** t * r * sum_scores\n",
    "        \n",
    "        grad += sum_rew\n",
    "    \n",
    "    return grad / len(rollouts)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can speedup the implementation, by precomputing the cumulative sum of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpomdp(rollouts, policy, gamma):\n",
    "    \n",
    "    grad = 0\n",
    "    \n",
    "    # A little more efficient implementation!\n",
    "    for roll in rollouts:\n",
    "        H = len(roll)\n",
    "        disc_rew = np.zeros((H, 1))\n",
    "        scores = np.zeros((H, policy.dim))\n",
    "        \n",
    "        for t in range(H):\n",
    "            s, a, r = roll[t]\n",
    "            disc_rew[t] = gamma ** t * r\n",
    "            scores[t] = policy.grad_log(s, a)\n",
    "        \n",
    "        cum_scores = np.cumsum(scores, axis=0)\n",
    "        \n",
    "        grad += np.sum(cum_scores * disc_rew, axis=0)\n",
    "        \n",
    "\n",
    "    return grad / len(rollouts)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our Implementation\n",
    "\n",
    "We test our G(PO)MDP implementation over the `MountainCarContinuous-v0` environment.\n",
    "\n",
    "MountainCarContinuous Environment Decription: [https://gym.openai.com/envs/MountainCarContinuous-v0/](https://gym.openai.com/envs/MountainCarContinuous-v0/)\n",
    "\n",
    "MountainCarContinuous Source Code: [https://github.com/openai/gym/blob/master/gym/envs/classic_control/continuous_mountain_car.py](https://github.com/openai/gym/blob/master/gym/envs/classic_control/continuous_mountain_car.py)\n",
    "\n",
    "We consider a modified simpler version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.envs.classic_control.continuous_mountain_car import Continuous_MountainCarEnv\n",
    "from gym.envs.registration import register\n",
    "\n",
    "\n",
    "class SimplifiedContinuous_MountainCarEnv(Continuous_MountainCarEnv):\n",
    "    \n",
    "    def __init__(self, goal_velocity=0):\n",
    "        super(SimplifiedContinuous_MountainCarEnv, self).__init__(goal_velocity)\n",
    "        \n",
    "        # We make the environment a little bit simpler by increasing the power\n",
    "        self.power =  0.02\n",
    "\n",
    "\n",
    "# Just in case you re-run the code\n",
    "if \"SimplifiedMountainCarContinuous-v1\" in gym.envs.registry.env_specs:\n",
    "    del gym.envs.registry.env_specs[\"SimplifiedMountainCarContinuous-v1\"]\n",
    "\n",
    "register(\n",
    "    id=\"SimplifiedMountainCarContinuous-v1\",\n",
    "    entry_point=\"__main__:SimplifiedContinuous_MountainCarEnv\",\n",
    "    max_episode_steps=200,\n",
    "    reward_threshold=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Instantiate the environment\n",
    "env = gym.make('SimplifiedMountainCarContinuous-v1')\n",
    "\n",
    "# Instantiate the policy\n",
    "policy = GaussianPolicy(env.observation_space.shape[0], std=0.2)\n",
    "\n",
    "gamma = 0.999  # discount factor\n",
    "m = 100        # number of trajectories per iteration\n",
    "K = 100        # maximum number of iterations\n",
    "alpha = 0.001  # learning rate\n",
    "T = 200        # lenght of each trajectory\n",
    "\n",
    "# Start training\n",
    "results = train(env, policy, gamma, m, K, alpha, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us render the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render('SimplifiedMountainCarContinuous-v1', policy, prefix='gpomdp')\n",
    "evaluate(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "01. Getting Started.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
