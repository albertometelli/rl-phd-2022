{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/albertometelli/rl-phd-2022/blob/main/01_getting_started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AoxOjIlOImwx"
   },
   "source": [
    "# Getting Started\n",
    "\n",
    "This notebook is inspired to the Stable Baselines3 tutorial available at [https://github.com/araffin/rl-tutorial-jnrr19](https://github.com/araffin/rl-tutorial-jnrr19).\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will learn how to use **Open AI Gym** environments and the basics of **stable baselines3**: how to instance an RL algorithm, train and evaluate it.\n",
    "\n",
    "### Links\n",
    "\n",
    "Open AI Gym Github: [https://github.com/openai/gym](https://github.com/openai/gym)\n",
    "\n",
    "Open AI Gym Documentation: [https://www.gymlibrary.ml](https://www.gymlibrary.ml)\n",
    "\n",
    "Stable Baselines 3 Github:[https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)\n",
    "\n",
    "Stable Baseline 3 Documentation: [https://stable-baselines3.readthedocs.io/en/master/](https://stable-baselines3.readthedocs.io/en/master/)\n",
    "\n",
    "## Install Dependencies and Stable Baselines3 Using Pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sp8rSS4DIhEV"
   },
   "outputs": [],
   "source": [
    "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3\n",
    "stable_baselines3.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Recording\n",
    "\n",
    "In Google Colab it is not possible to render the Gym environments, so we need to record a video and then reproduce it. Here are the helper functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up fake display; otherwise rendering will fail\n",
    "import os\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from IPython import display as ipythondisplay\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
    "os.environ['DISPLAY'] = ':1'\n",
    "\n",
    "def show_videos(video_path='', prefix=''):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/eleurent/highway-env\n",
    "\n",
    "    :param video_path: (str) Path to the folder containing videos\n",
    "    :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
    "    \"\"\"\n",
    "    html = []\n",
    "    for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "        html.append('''<video alt=\"{}\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                    </video>'''.format(mp4, video_b64.decode('ascii')))\n",
    "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
    "\n",
    "\n",
    "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
    "    \"\"\"\n",
    "    :param env_id: (str)\n",
    "    :param model: (RL model)\n",
    "    :param video_length: (int)\n",
    "    :param prefix: (str)\n",
    "    :param video_folder: (str)\n",
    "    \"\"\"\n",
    "    eval_env = DummyVecEnv([lambda: gym.make('CartPole-v1')])\n",
    "    # Start the video at step=0 and record 500 steps\n",
    "    eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
    "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
    "                              name_prefix=prefix)\n",
    "\n",
    "    obs = eval_env.reset()\n",
    "    for _ in range(video_length):\n",
    "        action, _ = model.predict(obs[0])\n",
    "        obs, _, _, _ = eval_env.step([action])\n",
    "\n",
    "    # Close the video recorder\n",
    "    eval_env.close()\n",
    "\n",
    "    \n",
    "def render(env_id, policy, video_length=500, prefix='', video_folder='videos/'):\n",
    "    record_video(env_id, policy, video_length, prefix, video_folder)\n",
    "    show_videos(video_folder, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "A helper function to plot the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_results(results):\n",
    "    plt.figure()\n",
    "    \n",
    "    for k in results.keys():\n",
    "        data = np.load(results[k] + '/evaluations.npz')\n",
    "        ts = data['timesteps']\n",
    "        res = data['results']\n",
    "        _mean, _std = res.mean(axis=1), res.std(axis=1)\n",
    "\n",
    "        plt.plot(ts, _mean, label=k)\n",
    "        plt.fill_between(ts, _mean-_std, _mean+_std, alpha=.2)\n",
    "        \n",
    "    plt.xlabel('Timesteps')\n",
    "    plt.ylabel('Average return')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Environments\n",
    "\n",
    "Initializing environments in Gym and is done as follows. We can find a list of available environment [here](https://gym.openai.com/envs/#classic_control)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n",
    "\n",
    "Cartpole Environment Decription: [https://gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/)\n",
    "\n",
    "Cartpole Source Code: [https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)\n",
    "\n",
    "![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the Environment\n",
    "\n",
    "We run an instance of `CartPole-v1` environment for 50 timesteps, showing the information returned by the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset() # resets the environment in the initial state\n",
    "print(\"Initial state: \", state)\n",
    "\n",
    "for _ in range(30): \n",
    "    action = env.action_space.sample() # sample a random action\n",
    "    \n",
    "    state, reward, done, _ = env.step(action)  # execute the action in the environment\n",
    "    print(\"State:\", state,\n",
    "          \"Action:\", action,\n",
    "          \"Reward:\", reward,\n",
    "          \"Done:\", done)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gym environment provides to this user mainly four methods:\n",
    "\n",
    "* `reset()`: resets the environment to its initial state $S_0 \\sim d_0$ and returns the observation corresponding to the initial state.\n",
    "\n",
    "\n",
    "* `step(action)`: takes an action $A_t$ as an input and executes the action in current state $S_t$ of the environment. This method returns a tuple of four values:\n",
    "\n",
    "    * `observation` (object): an environment-specific object representation of your observation of the environment after the action is executed. It corresponds to the observation of the next state $S_{t+1} \\sim p(\\cdot|S_t,A_t)$\n",
    "    \n",
    "    * `reward` (float): immediate reward $R_{t+1} = r(S_t,A_t)$ obtained by executing action $A_t$ in state $S_t$\n",
    "    \n",
    "    * `done`(boolean): whether the reached next state $S_{t+1}$ is a terminal state.\n",
    "    \n",
    "    * `info` (dict): additional information useful for debugging and environment-specific.\n",
    "    \n",
    "    \n",
    "*  `render(method='human')`: allows visualizing the agent in action. Note that graphical interface does not work on google colab, so we cannot use it directly (we will need a workaround).\n",
    "\n",
    "\n",
    "*  `seed()`: sets the seed for this environmentâ€™s random number generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation and Action Spaces\n",
    "\n",
    "*  `observation_space`: this attribute provides the format of valid observations $\\mathcal{S}$. It is of datatype `Space` provided by Gym. For example, if the observation space is of type `Box` and the shape of the object is `(4,)`, this denotes a valid observation will be an array of 4 numbers.\n",
    "\n",
    "*  `action_space`: this attribute provides the format of valid actions $\\mathcal{A}$. It is of datatype `Space` provided by Gym. For example, if the action space is of type `Discrete` and gives the value `Discrete(2)`, this means there are two valid discrete actions: 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "\n",
    "print(env.action_space)\n",
    "\n",
    "print(env.observation_space.high)\n",
    "\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Spaces` types available in Gym:\n",
    "\n",
    "*  `Box`: an $n$-dimensional compact space (i.e., a compact subset of $\\mathbb{R}^n$). The bounds of the space are contained in the `high` and `low` attributes.\n",
    "\n",
    "\n",
    "*  `Discrete`: a discrete space made of $n$ elements, where $\\{0,1,\\dots,n-1\\}$ are the possible values.\n",
    "\n",
    "\n",
    "Other `Spaces` types can be used: `Dict`, `Tuple`, `MultiBinary`, `MultiDiscrete`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym.spaces import Box, Discrete\n",
    "\n",
    "observation_space = Box(low=-1.0, high=2.0, shape=(3,), dtype=np.float32)\n",
    "print(observation_space.sample())\n",
    "\n",
    "observation_space = Discrete(4)\n",
    "print(observation_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details on the Cartpole Environment \n",
    "\n",
    "From [https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)\n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
    "\n",
    "### Action Space\n",
    "The action space is `action` in $\\{0,1\\}$, where `action` is used to push the cart with a fixed amount of force:\n",
    "\n",
    " | Num | Action                 |\n",
    "    |-----|------------------------|\n",
    "    | 0   | Push cart to the left  |\n",
    "    | 1   | Push cart to the right |\n",
    "    \n",
    "Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it.\n",
    "    \n",
    "### Observation Space\n",
    "The observation is a `ndarray` with shape `(4,)` where the elements correspond to the following:\n",
    "\n",
    "   | Num | Observation           | Min                  | Max                |\n",
    "    |-----|-----------------------|----------------------|--------------------|\n",
    "    | 0   | Cart Position         | -4.8*                | 4.8*                |\n",
    "    | 1   | Cart Velocity         | -Inf                 | Inf                |\n",
    "    | 2   | Pole Angle            | ~ -0.418 rad (-24Â°)**| ~ 0.418 rad (24Â°)** |\n",
    "    | 3   | Pole Angular Velocity | -Inf                 | Inf                |\n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
    "\n",
    "**Note:** above denotes the ranges of possible observations for each element, but in two cases this range exceeds the range of possible values in an un-terminated episode:\n",
    "- `*`: the cart x-position can be observed between `(-4.8, 4.8)`, but an episode terminates if the cart leaves the `(-2.4, 2.4)` range.\n",
    "- `**`: Similarly, the pole angle can be observed between  `(-.418, .418)` radians or precisely **Â±24Â°**, but an episode is  terminated if the pole angle is outside the `(-.2095, .2095)` range or precisely **Â±12Â°**\n",
    "    \n",
    "### Rewards\n",
    "Reward is 1 for every step taken, including the termination step.\n",
    "\n",
    "### Starting State\n",
    "All observations are assigned a uniform random value between (-0.05, 0.05)\n",
    "\n",
    "### Episode Termination\n",
    "The episode terminates of one of the following occurs:\n",
    "1. Pole Angle is more than Â±12Â°\n",
    "2. Cart Position is more than Â±2.4 (center of the cart reaches the edge of the display)\n",
    "3. Episode length is greater than 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of some Simple Policies\n",
    "\n",
    "We now evaluate some policies on the cartpole.\n",
    "\n",
    "* **Uniform Policy**: uniformly random policy\n",
    "\n",
    "$$\n",
    "\\pi(a|s) = \\mathrm{Uni}(\\{0,1\\})\n",
    "$$\n",
    "\n",
    "* **Reactive Policy**: simple deterministic policy that selects the action based on the pole angle\n",
    "\n",
    "$$\n",
    "\\pi(s) = \\begin{cases}\n",
    "                0 & \\text{if Pole Angle } \\le 0 \\\\\n",
    "                1 & \\text{otherwise}\n",
    "            \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformPolicy:\n",
    "    \n",
    "    def predict(self, obs):\n",
    "        return np.random.randint(0, 2), obs  # return the observation to comply with stable-baselines3\n",
    "\n",
    "\n",
    "class ReactivePolicy:\n",
    "    \n",
    "    def predict(self, obs):\n",
    "        if obs[2] <= 0:\n",
    "            return 0, obs\n",
    "        else:\n",
    "            return 1, obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a function to evaluate the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, policy, gamma=1., num_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param env: (Env object) the Gym environment\n",
    "    :param policy: (BasePolicy object) the policy in stable_baselines3\n",
    "    :param gamma: (float) the discount factor\n",
    "    :param num_episodes: (int) number of episodes to evaluate it\n",
    "    :return: (float) Mean reward for the last num_episodes\n",
    "    \"\"\"\n",
    "    discounter = 1.\n",
    "    all_episode_rewards = []\n",
    "    for i in range(num_episodes): # iterate over the episodes\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done: # iterate over the steps until termination\n",
    "            action, _ = policy.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards.append(reward * discounter) # compute discounted reward\n",
    "            discounter *= gamma\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    std_episode_reward = np.std(all_episode_rewards) / np.sqrt(num_episodes - 1)\n",
    "    print(\"Mean reward:\", mean_episode_reward, \n",
    "          \"Std reward:\", std_episode_reward,\n",
    "          \"Num episodes:\", num_episodes)\n",
    "\n",
    "    return mean_episode_reward, std_episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the uniform policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_policy = UniformPolicy()\n",
    "\n",
    "uniform_policy_mean, uniform_policy_std = evaluate(env, uniform_policy)\n",
    "\n",
    "render('CartPole-v1', uniform_policy, prefix='cartpole-uniform_policy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the reactive policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reactive_policy = ReactivePolicy()\n",
    "\n",
    "reactive_policy_mean, reactive_policy_std = evaluate(env, reactive_policy)\n",
    "\n",
    "render('CartPole-v1', reactive_policy, prefix='cartpole-reactive_policy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Training\n",
    "\n",
    "We now use stable-baselines3 to train some simple algorithms. We start by using [Proximal Policy Optimization](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html).\n",
    "\n",
    "We select the MlpPolicy, that is an alias of [ActorCriticPolicy](https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/policies.py), because the state of the CartPole environment is a feature vector (not images for instance). The type of action to use (discrete/continuous) will be automatically deduced from the environment action space.\n",
    "\n",
    "We consider two network architectures:\n",
    "\n",
    "* Linear policy\n",
    "* Two hidden layers of 32 neurons each policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "\n",
    "# Instantiate the algorithm with 32x32 NN approximator for both actor and critic\n",
    "ppo_mlp = PPO(\"MlpPolicy\", env, verbose=1, \n",
    "                learning_rate=0.01,\n",
    "                policy_kwargs=dict(net_arch = [dict(pi=[32, 32], vf=[32, 32])]))\n",
    "\n",
    "print(ppo_mlp.policy)\n",
    "\n",
    "# Instantiate the algorithm with linear approximator for both actor and critic\n",
    "ppo_linear = PPO(\"MlpPolicy\", env, verbose=1, \n",
    "                   learning_rate=0.01,\n",
    "                   policy_kwargs=dict(net_arch = [dict(pi=[], vf=[])]))\n",
    "\n",
    "print(ppo_linear.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now train the algorithms. In order to keep track of the performance during learning, we use an [EvalCallback](https://stable-baselines.readthedocs.io/en/master/guide/callbacks.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate evaluation env\n",
    "eval_env = gym.make('CartPole-v1')\n",
    "\n",
    "# Train the agent for 50000 steps\n",
    "ppo_mlp.learn(total_timesteps=50000, eval_freq=2048, eval_env=eval_env,\n",
    "              eval_log_path='./logs/cartpole/ppo_mlp', log_interval=4)\n",
    "\n",
    "ppo_linear.learn(total_timesteps=50000, eval_freq=2048, eval_env=eval_env, \n",
    "              eval_log_path='./logs/cartpole/ppo_linear', log_interval=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'PPO-MLP': './logs/cartpole/ppo_mlp',\n",
    "           'PPO-LINEAR': './logs/cartpole/ppo_linear',}\n",
    "        \n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained models\n",
    "ppo_mlp_mean, ppo_mlp_std = evaluate(env, ppo_mlp)\n",
    "render('CartPole-v1', ppo_mlp, prefix='ppo_mlp')\n",
    "\n",
    "ppo_linear_mean, ppo_linear_std = evaluate(env, ppo_linear)\n",
    "render('CartPole-v1', ppo_linear, prefix='ppo_linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the weights learned by PPO with the linear policy. Since actions are discrete, the policy model is **softmax**:\n",
    "\n",
    "$$\n",
    "\\pi_{\\boldsymbol{\\theta}}(a|\\mathbf{s}) \\propto \\exp \\left( \\mathbf{s}^T \\boldsymbol{\\theta}(a) + b(a) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ppo_linear.policy.action_net.weight)\n",
    "print(ppo_linear.policy.action_net.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try [DQN](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) with an MlpPolicy as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# Instantiate the algorithm with 32x32 NN approximator for both actor and critic\n",
    "dqn_mlp = DQN(\"MlpPolicy\", env, verbose=1, \n",
    "                learning_starts=3000,\n",
    "                policy_kwargs=dict(net_arch = [32, 32], activation_fn=nn.Tanh))\n",
    "\n",
    "print(dqn_mlp.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent for 50000 steps\n",
    "dqn_mlp.learn(total_timesteps=50000, eval_freq=2048, eval_env=eval_env, \n",
    "              eval_log_path='./logs/cartpole/dqn_mlp', log_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained models\n",
    "dqn_mlp_mean, dqn_mlp_std = evaluate(env, dqn_mlp)\n",
    "render('CartPole-v1', dqn_mlp, prefix='dqn_mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now plot the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the training curves\n",
    "\n",
    "results['DQN'] = './logs/cartpole/dqn_mlp'\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the results\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "algs = ['Random', 'Reactive', 'PPO MLP', 'PPO Linear', 'DQN']\n",
    "means = [uniform_policy_mean, reactive_policy_mean, ppo_mlp_mean, ppo_linear_mean, dqn_mlp_mean]\n",
    "errors = [uniform_policy_std, reactive_policy_std, ppo_mlp_std, ppo_linear_std, dqn_mlp_std]\n",
    "\n",
    "ax.bar(algs, means, yerr=errors, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "01. Getting Started.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
