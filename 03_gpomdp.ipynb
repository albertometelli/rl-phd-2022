{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/albertometelli/rl-phd-2020/01_intro_gym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AoxOjIlOImwx"
   },
   "source": [
    "# G(PO)MDP\n",
    "\n",
    "This notebook is inspired to the Stable Baselines3 tutorial available at [https://github.com/araffin/rl-tutorial-jnrr19](https://github.com/araffin/rl-tutorial-jnrr19).\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will implement a very basic version of **G(PO)MDP**.\n",
    "\n",
    "### Links\n",
    "\n",
    "Open AI Gym Github: [https://github.com/openai/gym](https://github.com/openai/gym)\n",
    "\n",
    "Open AI Gym Documentation: [https://www.gymlibrary.ml](https://www.gymlibrary.ml)\n",
    "\n",
    "Stable Baselines 3 Github:[https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)\n",
    "\n",
    "Stable Baseline 3 Documentation: [https://stable-baselines3.readthedocs.io/en/master/](https://stable-baselines3.readthedocs.io/en/master/)\n",
    "\n",
    "## Install Dependencies and Stable Baselines3 Using Pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sp8rSS4DIhEV"
   },
   "outputs": [],
   "source": [
    "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stable_baselines3\n",
    "stable_baselines3.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "A helper function to evaluate policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, policy, gamma=1., num_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param env: (Env object) the Gym environment\n",
    "    :param policy: (BasePolicy object) the policy in stable_baselines3\n",
    "    :param gamma: (float) the discount factor\n",
    "    :param num_episodes: (int) number of episodes to evaluate it\n",
    "    :return: (float) Mean reward for the last num_episodes\n",
    "    \"\"\"\n",
    "    discounter = 1.\n",
    "    all_episode_rewards = []\n",
    "    for i in range(num_episodes): # iterate over the episodes\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done: # iterate over the steps until termination\n",
    "            action, _ = policy.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards.append(reward * discounter) # compute discounted reward\n",
    "            discounter *= gamma\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    std_episode_reward = np.std(all_episode_rewards) / np.sqrt(num_episodes - 1)\n",
    "    print(\"Mean reward:\", mean_episode_reward, \n",
    "          \"Std reward:\", std_episode_reward,\n",
    "          \"Num episodes:\", num_episodes)\n",
    "\n",
    "    return mean_episode_reward, std_episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "A helper function to plot the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_results(results):\n",
    "    plt.figure()\n",
    "    \n",
    "    for k in results.keys():\n",
    "        data = np.load(results[k] + '/evaluations.npz')\n",
    "        ts = data['timesteps']\n",
    "        res = data['results']\n",
    "        _mean, _std = res.mean(axis=1), res.std(axis=1)\n",
    "\n",
    "        plt.plot(ts, _mean, label=k)\n",
    "        plt.fill_between(ts, _mean-_std, _mean+_std, alpha=.2)\n",
    "        \n",
    "    plt.xlabel('Timesteps')\n",
    "    plt.ylabel('Average return')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G(PO)MDP\n",
    "\n",
    "![ss](gpomdp.png)\n",
    "\n",
    "**References**\n",
    "\n",
    "Baxter, Jonathan, and Peter L. Bartlett. \"Infinite-horizon policy-gradient estimation.\" Journal of Artificial Intelligence Research 15 (2001): 319-350."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "\n",
    "We will use a Gaussian policy, linear in the state variables and with fixed (non-learnable) standard deviation. \n",
    "\n",
    "$$\n",
    "\\pi_{\\boldsymbol{\\theta}}(a|\\mathbf{s}) = \\mathcal{N}(a| \\boldsymbol{\\theta}^T \\mathbf{s}, \\sigma^2)\n",
    "$$\n",
    "\n",
    "The policy must implement the usual `predict` method and some additional methods for computing the policy gradient. Specifically, we will need a `grad_log` method to return the gradient of the logarithm of the policy (the score):\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol{\\theta}} \\log \\pi_{\\boldsymbol{\\theta}}(a|\\mathbf{s})= \\frac{(a - \\boldsymbol{\\theta}^T \\mathbf{s})\\mathbf{s}}{\\sigma^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPolicy:\n",
    "    \n",
    "    def __init__(self, dim, std=0.1):\n",
    "        \"\"\"\n",
    "        :param dim: number of state variables\n",
    "        :param std: fixed standard deviation\n",
    "        \"\"\"\n",
    "        \n",
    "        self.std = std\n",
    "        self.dim = dim\n",
    "        self.theta = np.zeros((dim,))  # zero initializatoin\n",
    "    \n",
    "    def get_theta(self):\n",
    "        return self.theta\n",
    "    \n",
    "    def set_theta(self, value):\n",
    "        self.theta = value\n",
    "    \n",
    "    def predict(self, obs):\n",
    "        mean = np.dot(obs, self.theta)\n",
    "        action = mean + np.random.randn() * self.std\n",
    "        return np.array([action]), obs\n",
    "    \n",
    "    def grad_log(self, obs, action):\n",
    "        mean = np.dot(obs, self.theta)\n",
    "        return (action - mean) * obs / self.std ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine\n",
    "\n",
    "We provide the already implemented skeleton of the training routine that samples at every iterations $m$ trajectories from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_rollouts(env, policy, m, T):\n",
    "    \"\"\"\n",
    "    Collects m rollouts by running the policy in the\n",
    "        environment\n",
    "    :param env: (Env object) the Gym environment\n",
    "    :param policy: (Policy object) the policy\n",
    "    :param gamma: (float) the discount factor\n",
    "    :param m: (int) number of episodes per iterations\n",
    "    :param K: (int) maximum number of iterations\n",
    "    :param theta0: (ndarray) initial parameters (d,)\n",
    "    :param alpha: (float) the constant learning rate\n",
    "    :param T: (int) the trajectory horizon\n",
    "    :return: (list of lists) one list per episode\n",
    "                each containing triples (s, a, r)\n",
    "    \"\"\"\n",
    "    \n",
    "    ll = []\n",
    "    for j in range(m):\n",
    "        s = env.reset()\n",
    "        t = 0\n",
    "        done = False\n",
    "        l = []\n",
    "        while t < T and not done:\n",
    "            a, _ = policy.predict(s)\n",
    "            s1, r, done, _ = env.step(a)\n",
    "            l.append((s, a, r))\n",
    "            s = s1\n",
    "        ll.append(l)\n",
    "    return ll\n",
    "            \n",
    "def train(env, policy, gamma, m, K, alpha, T):\n",
    "    \"\"\"\n",
    "    Train a policy with G(PO)MDP\n",
    "    :param env: (Env object) the Gym environment\n",
    "    :param policy: (Policy object) the policy\n",
    "    :param gamma: (float) the discount factor\n",
    "    :param m: (int) number of episodes per iterations\n",
    "    :param K: (int) maximum number of iterations\n",
    "    :param alpha: (float) the constant learning rate\n",
    "    :param T: (int) the trajectory horizon\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    for k in range(K):\n",
    "        \n",
    "        print('Iteration:', k)\n",
    "        \n",
    "        #Generate rollouts\n",
    "        rollouts = collect_rollouts(env, policy, m, T)\n",
    "        \n",
    "        #Get policy parameter\n",
    "        theta = policy.get_theta()\n",
    "        print('\\tParameters:', theta)\n",
    "        \n",
    "        #Call your G(PO)MDP estimator\n",
    "        pg = gpomdp(rollouts, policy, gamma)\n",
    "        \n",
    "        # Update policy parameter\n",
    "        theta = theta + alpha * pg\n",
    "        \n",
    "        #Set policy parameters\n",
    "        policy.set_theta(theta)\n",
    "        \n",
    "        evaluate(env, policy, gamma)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a possible implementation of G(PO)MDP that turns out to be very inefficient! It can be improved by precomputing the scores at the beginning and using matrix opertions instead of for cicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpomdp(rollouts, policy, gamma):\n",
    "    \n",
    "    grad = 0\n",
    "    \n",
    "    # Very very inefficient implementation!\n",
    "    for roll in rollouts:\n",
    "        H = len(roll)\n",
    "        \n",
    "        sum_rew = 0.\n",
    "        for t in range(H):\n",
    "            \n",
    "            sum_scores = 0.\n",
    "            for l in range(t + 1):\n",
    "                s, a, _ = roll[l]\n",
    "                score = policy.grad_log(s, a)\n",
    "                sum_scores += score\n",
    "            \n",
    "            _, _, r = roll[t]\n",
    "            sum_rew += gamma ** t * r * sum_scores\n",
    "        \n",
    "        grad += sum_rew\n",
    "    \n",
    "    return grad / len(rollouts)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "\tParameters: [0. 0. 0.]\n",
      "Mean reward: -7.859991506673208 Std reward: 6.7157023584383175 Num episodes: 100\n",
      "Iteration: 1\n",
      "\tParameters: [-0.18944941  0.26758589 -1.2382909 ]\n",
      "Mean reward: -9.01409086678231 Std reward: 7.9114032928919045 Num episodes: 100\n",
      "Iteration: 2\n",
      "\tParameters: [-0.37598829 -0.06157708 -1.86202814]\n",
      "Mean reward: -8.781609984884277 Std reward: 7.726040507408961 Num episodes: 100\n",
      "Iteration: 3\n",
      "\tParameters: [-0.89099668 -0.16507964 -1.52975174]\n",
      "Mean reward: -9.00720260828952 Std reward: 7.939470785957481 Num episodes: 100\n",
      "Iteration: 4\n",
      "\tParameters: [-1.10388617 -0.28137463 -1.43747972]\n",
      "Mean reward: -8.559273356910387 Std reward: 7.48193912459331 Num episodes: 100\n",
      "Iteration: 5\n",
      "\tParameters: [-1.98712175 -0.4373299  -1.68586476]\n",
      "Mean reward: -7.940612329768299 Std reward: 6.892122410503793 Num episodes: 100\n",
      "Iteration: 6\n",
      "\tParameters: [-2.30793693 -0.55096147 -1.83268079]\n",
      "Mean reward: -8.011813692885303 Std reward: 7.0128580593471295 Num episodes: 100\n",
      "Iteration: 7\n",
      "\tParameters: [-2.68687904 -0.65686699 -2.1541423 ]\n",
      "Mean reward: -7.939478453830751 Std reward: 6.960385269445113 Num episodes: 100\n",
      "Iteration: 8\n",
      "\tParameters: [-2.87923947 -0.92635547 -2.89463384]\n",
      "Mean reward: -7.451164081242403 Std reward: 6.42635421889408 Num episodes: 100\n",
      "Iteration: 9\n",
      "\tParameters: [-2.70084122 -0.45555362 -1.73767258]\n",
      "Mean reward: -8.197768364411926 Std reward: 7.19062519918579 Num episodes: 100\n",
      "Iteration: 10\n",
      "\tParameters: [-2.17158458 -0.27259293 -2.10212567]\n",
      "Mean reward: -6.833216661698024 Std reward: 5.810605812356159 Num episodes: 100\n",
      "Iteration: 11\n",
      "\tParameters: [-1.80170959 -0.13172688 -1.07096283]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Instantiate the environment\n",
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "#Instantiate the policy\n",
    "policy = GaussianPolicy(env.observation_space.shape[0], std=2)\n",
    "\n",
    "#Other parameters\n",
    "gamma = 0.99\n",
    "m = 20\n",
    "K = 20\n",
    "alpha = 0.001\n",
    "T = 100\n",
    "\n",
    "train(env, policy, gamma, m, K, alpha, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "01. Getting Started.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
