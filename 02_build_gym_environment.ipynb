{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/albertometelli/rl-phd-2020/01_intro_gym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AoxOjIlOImwx"
   },
   "source": [
    "# Build a Gym Environment\n",
    "\n",
    "This notebook is inspired to the Stable Baselines3 tutorial available at [https://github.com/araffin/rl-tutorial-jnrr19](https://github.com/araffin/rl-tutorial-jnrr19).\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will learn how to build a customized environment with **Open AI Gym**.\n",
    "\n",
    "### Links\n",
    "\n",
    "Open AI Gym Github: [https://github.com/openai/gym](https://github.com/openai/gym)\n",
    "\n",
    "Open AI Gym Documentation: [https://www.gymlibrary.ml](https://www.gymlibrary.ml)\n",
    "\n",
    "Stable Baselines 3 Github:[https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)\n",
    "\n",
    "Stable Baseline 3 Documentation: [https://stable-baselines3.readthedocs.io/en/master/](https://stable-baselines3.readthedocs.io/en/master/)\n",
    "\n",
    "## Install Dependencies and Stable Baselines3 Using Pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sp8rSS4DIhEV"
   },
   "outputs": [],
   "source": [
    "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stable_baselines3\n",
    "stable_baselines3.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "A helper function to evaluate policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, policy, gamma=1., num_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param env: (Env object) the Gym environment\n",
    "    :param policy: (BasePolicy object) the policy in stable_baselines3\n",
    "    :param gamma: (float) the discount factor\n",
    "    :param num_episodes: (int) number of episodes to evaluate it\n",
    "    :return: (float) Mean reward for the last num_episodes\n",
    "    \"\"\"\n",
    "    discounter = 1.\n",
    "    all_episode_rewards = []\n",
    "    for i in range(num_episodes): # iterate over the episodes\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done: # iterate over the steps until termination\n",
    "            action, _ = policy.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards.append(reward * discounter) # compute discounted reward\n",
    "            discounter *= gamma\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    std_episode_reward = np.std(all_episode_rewards) / np.sqrt(num_episodes - 1)\n",
    "    print(\"Mean reward:\", mean_episode_reward, \n",
    "          \"Std reward:\", std_episode_reward,\n",
    "          \"Num episodes:\", num_episodes)\n",
    "\n",
    "    return mean_episode_reward, std_episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "A helper function to plot the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_results(results):\n",
    "    plt.figure()\n",
    "    \n",
    "    for k in results.keys():\n",
    "        data = np.load(results[k] + '/evaluations.npz')\n",
    "        ts = data['timesteps']\n",
    "        res = data['results']\n",
    "        _mean, _std = res.mean(axis=1), res.std(axis=1)\n",
    "\n",
    "        plt.plot(ts, _mean, label=k)\n",
    "        plt.fill_between(ts, _mean-_std, _mean+_std, alpha=.2)\n",
    "        \n",
    "    plt.xlabel('Timesteps')\n",
    "    plt.ylabel('Average return')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dam Environment\n",
    "\n",
    "The `Dam` environment models a lake water reservoir where the objective is to learn a per-day water **release** policy that meets a given **demand** while keeping the water level below a **flooding** threshold. The reservoir dynamics is governed by the mass-conservation equation:\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t + i_t - a_t \\qquad \\text{with} \\qquad t \\in \\{0,\\dots,359\\},\n",
    "$$\n",
    "\n",
    "where $x_t$ is the storage volume of the water reservoir at the beginning of day $t$, $i_t$ is the net inflow volume during day $t$, and $a_{t}$ is the release volume during day $t$. The state of the environment is represented by the pair $s_t=(x_t,i_{t-1})^\\mathtt{T}$. The inflow $i_t$ is a memoryless cyclostationary stochstic process, governed by the equation:\n",
    "\n",
    "$$\n",
    "i_t = \\overline{i}_t + \\epsilon_t \\qquad \\text{where} \\qquad \\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2_i=2.0) \\qquad \\text{and} \\qquad  \\overline{i}_t = 6 + 8 \\begin{cases}\n",
    "                         3 \\sin\\left( \\frac{3 \\pi t}{359} \\right)   & \\text{if } 0 \\le t < 120 \\\\\n",
    "                         0.25 \\sin\\left( \\frac{3 \\pi t}{359} \\right)  & \\text{if } 120 \\le t < 240 \\\\\n",
    "                         0.5 \\sin\\left( \\frac{3 \\pi t}{359} \\right)               & \\text{otherwise}\n",
    "                                    \\end{cases}\n",
    "$$\n",
    "\n",
    "The water reservoir is supposed to have capacity of $X_{\\max} = 500\\, Mm^3$, minimum storage of $X_{\\min}=50\\, Mm^3$. A **flooding** occurs when the storage volume is above $X_{\\text{flood}}=300\\, Mm^3$. Thus, if we perform actions that if executed violate these constraint, they have to be clipped. The per-day **demand** is $A_{\\text{demand}} = 10 Mm^3$. The reward function penalizes floodings and demands that are not satisfied:\n",
    "\n",
    "$$\n",
    "r(s,a) = -\\alpha \\max\\{0, x_t - X_{\\text{flood}}\\} - \\beta \\max\\{0, A_{\\text{demand}} - a_t\\}^2,\n",
    "$$\n",
    "\n",
    "where $\\alpha=\\beta=0.5$.\n",
    "\n",
    "**References**\n",
    "\n",
    "Castelletti, A., Stefano Galelli, Marcello Restelli, and Rodolfo Soncini‐Sessa. \"Tree‐based reinforcement learning for optimal water reservoir operation.\" Water Resources Research 46, no. 9 (2010).\n",
    "\n",
    "\n",
    "Parisi, Simone, Matteo Pirotta, Nicola Smacchia, Luca Bascetta, and Marcello Restelli. \"Policy gradient approaches for multi-objective sequential decision making.\" In 2014 International Joint Conference on Neural Networks (IJCNN), pp. 2323-2330. IEEE, 2014.\n",
    "\n",
    "\n",
    "Tirinzoni, Andrea, Andrea Sessa, Matteo Pirotta, and Marcello Restelli. \"Importance weighted transfer of samples in reinforcement learning.\" In International Conference on Machine Learning, pp. 4936-4945. PMLR, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym.spaces import Box\n",
    "\n",
    "class Dam(gym.Env):\n",
    "    \"\"\"\n",
    "    The Dam problem.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha = 0.5, beta = 0.5):\n",
    "        super(Dam, self).__init__()\n",
    "        \n",
    "        self.DEMAND = 10.0  # Water demand -> At least DEMAND/day must be supplied or a cost is incurred\n",
    "        self.FLOODING = 300.0  # Flooding threshold -> No more than FLOODING can be stored or a cost is incurred\n",
    "        self.MIN_STORAGE = 50.0 # Minimum storage capacity -> At most max{S - MIN_STORAGE, 0} must be released\n",
    "        self.MAX_STORAGE = 500.0  # Maximum storage capacity -> At least max{S - MAX_STORAGE, 0} must be released\n",
    "        self.INFLOW_STD = 2.0 # Random inflow std\n",
    "        self.MAX_INFLOW = 30.0\n",
    "        \n",
    "        assert alpha + beta == 1.0 # Check correctness\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.action_space = Box(low=np.array([0]),\n",
    "                                high=np.array([self.MAX_STORAGE]),\n",
    "                                dtype=np.float32)\n",
    "        \n",
    "        self.observation_space = Box(low=np.array([0, 0]),\n",
    "                                     high=np.array([self.MAX_STORAGE, self.MAX_INFLOW]),\n",
    "                                     dtype=np.float32)\n",
    "\n",
    "                                                           \n",
    "    def _inflow(self, day):\n",
    "        if day < 120:\n",
    "            inflow_mean = 3 * np.sin(day * 3 * np.pi / 359)\n",
    "        elif day < 240:\n",
    "            inflow_mean = 0.5 * np.sin(day * 3 * np.pi / 359)\n",
    "        else:\n",
    "            inflow_mean = 0.25 * np.sin(day * 3 * np.pi / 359)\n",
    "        \n",
    "        return 6 + 8 * inflow_mean\n",
    "                                                           \n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.day = np.random.randint(0, 360)\n",
    "        self.inflow = self._inflow(self.day)\n",
    "        self.storage = np.random.uniform(self.MIN_STORAGE, self.MAX_STORAGE)\n",
    "                                                           \n",
    "        return np.array([self.storage, self.inflow]).astype(np.float32)\n",
    "\n",
    "                                                           \n",
    "    def step(self, action):\n",
    "        \n",
    "        action = float(action)\n",
    "        \n",
    "        # Bound the action\n",
    "        actionLB = max(self.storage - self.MAX_STORAGE, 0.0)\n",
    "        actionUB = max(self.storage - self.MIN_STORAGE, 0.0)\n",
    "        bounded_action = min(max(action, actionLB), actionUB)\n",
    "                                                           \n",
    "        # Transition dynamics\n",
    "        action = bounded_action\n",
    "        nextinflow = self._inflow(self.day) + np.random.randn() * self.INFLOW_STD\n",
    "        nextstorage = max(self.storage + nextinflow - action, 0.0)\n",
    "\n",
    "        # Cost due to the excess level wrt the flooding threshold\n",
    "        reward_flooding = -max(self.storage - self.FLOODING, 0.0) / 4\n",
    "\n",
    "        # Deficit in the water supply wrt the water demand\n",
    "        reward_demand = -max(self.DEMAND - action, 0.0) ** 2\n",
    "        \n",
    "        # The final reward is a weighted average of the two costs\n",
    "        reward = self.alpha * reward_flooding + self.beta * reward_demand\n",
    "\n",
    "        # Get next day\n",
    "        nextday = self.day + 1 if self.day < 360 else 1\n",
    "                                                           \n",
    "        self.storage = nextstorage\n",
    "        self.inflow = nextinflow\n",
    "        self.day = nextday\n",
    "\n",
    "        return np.array([self.storage, self.inflow]).astype(np.float32), reward, False, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to instance the environment with `gym.make`, we need to register the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "\n",
    "register(\n",
    "    id=\"Dam-v1\",\n",
    "    entry_point=\"__main__:Dam\",\n",
    "    max_episode_steps=500,\n",
    "    reward_threshold=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the environment\n",
    "\n",
    "Stable Baselines3 provides a [helper](https://stable-baselines3.readthedocs.io/en/master/common/env_checker.html) to check that our environment complies with the Gym interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/alberto/.local/lib/python3.8/site-packages/stable_baselines3/common/env_checker.py:272: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = Dam()\n",
    "\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look to the inflow profile as a function of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "days = np.arange(0, 360)\n",
    "inflow = [env._inflow(d) for d in days]\n",
    "\n",
    "plt.plot(days, inflow)\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Inflow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate some simple Policies\n",
    "\n",
    "* **No-release policy**: a policy that releases the minimum allowed amount of water\n",
    "\n",
    "$$\n",
    "\\pi(s) = 0\n",
    "$$\n",
    "\n",
    "\n",
    "* **All-release policy**: a policy that releases the maximum allowed amount of water\n",
    "\n",
    "$$\n",
    "\\pi(s) = +\\infty\n",
    "$$\n",
    "\n",
    "\n",
    "* **Zero-mean Gaussian policy**: a policy that selects the action sampled from a Gaussian policy with zero mean and variance $\\sigma^2=1$\n",
    "\n",
    "$$\n",
    "\\pi(a|s) = \\mathcal{N}(0,\\sigma^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoReleasePolicy():\n",
    "    \n",
    "    def predict(self, obs):\n",
    "        return 0, obs\n",
    "\n",
    "\n",
    "class AllReleasePolicy():\n",
    "    \n",
    "    def predict(self, obs):\n",
    "        return np.inf, obs\n",
    "    \n",
    "\n",
    "class ZeroMeanGaussianPolicy():\n",
    "    \n",
    "    def predict(self, obs):\n",
    "        return np.random.randn(), obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: -18882.703070834657 Std reward: 90.95518948485874 Num episodes: 100\n",
      "Mean reward: -4984.202620733882 Std reward: 98.52234358579692 Num episodes: 100\n",
      "Mean reward: -18742.816175026983 Std reward: 80.85152357721691 Num episodes: 100\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Dam-v1\")\n",
    "\n",
    "no_release_policy = NoReleasePolicy()\n",
    "\n",
    "all_release_policy = AllReleasePolicy()\n",
    "\n",
    "gauss_policy = ZeroMeanGaussianPolicy()\n",
    "\n",
    "\n",
    "no_release_mean, no_release_std = evaluate(env, no_release_policy)\n",
    "all_release_mean, all_release_std = evaluate(env, all_release_policy)\n",
    "gauss_policy_mean, gauss_policy_std = evaluate(env, gauss_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train PPO, DDPG, and SAC\n",
    "\n",
    "We now train three algorithms suitable for environments with continuous actions: [Proximal Policy Optimization](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html), [Deep Deterministic Policy Gradient](https://stable-baselines3.readthedocs.io/en/master/modules/ddpg.html), and [Soft Actor Critic](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "PPO\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Eval num_timesteps=2048, episode_reward=-18660.60 +/- 1003.82\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.87e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=-19411.84 +/- 915.25\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | -1.94e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004268082 |\n",
      "|    clip_fraction        | 0.0167      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 4.93e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.77e+05    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.000882   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.72e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6144, episode_reward=-18491.95 +/- 1076.48\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.85e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040316302 |\n",
      "|    clip_fraction        | 0.0119       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -2.15e-06    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.15e+05     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000489    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.93e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8192, episode_reward=-18684.15 +/- 752.04\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.87e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032210564 |\n",
      "|    clip_fraction        | 0.0143       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.94e+05     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000826    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.96e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.86e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 810       |\n",
      "|    iterations      | 4         |\n",
      "|    time_elapsed    | 10        |\n",
      "|    total_timesteps | 8192      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10240, episode_reward=-18439.04 +/- 395.76\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | -1.84e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003987023 |\n",
      "|    clip_fraction        | 0.0276      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.6e+05     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00165    |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 3.48e+05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12288, episode_reward=-19165.01 +/- 1011.51\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.92e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009578123 |\n",
      "|    clip_fraction        | 0.00317      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.83e+05     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -5.85e-05    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.9e+05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=14336, episode_reward=-19167.04 +/- 452.53\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.92e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047990754 |\n",
      "|    clip_fraction        | 0.0191       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.89e+05     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.87e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16384, episode_reward=-18575.59 +/- 865.89\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | -1.86e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004838015 |\n",
      "|    clip_fraction        | 0.019       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.59e+05    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00134    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 3.43e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.85e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 806       |\n",
      "|    iterations      | 8         |\n",
      "|    time_elapsed    | 20        |\n",
      "|    total_timesteps | 16384     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18432, episode_reward=-18532.54 +/- 459.39\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | -1.85e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002416426 |\n",
      "|    clip_fraction        | 0.00791     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.72e+05    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.000577   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 3.59e+05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=20480, episode_reward=-19271.57 +/- 1156.31\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.93e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050784373 |\n",
      "|    clip_fraction        | 0.0147       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.07e+05     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 3.85e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=22528, episode_reward=-18175.69 +/- 396.42\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.82e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013258503 |\n",
      "|    clip_fraction        | 0.00483      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.03e+05     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | 0.000187     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.85e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24576, episode_reward=-18583.92 +/- 790.35\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.86e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053141373 |\n",
      "|    clip_fraction        | 0.0477       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.81e+05     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.0042      |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 3.91e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.87e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 775       |\n",
      "|    iterations      | 12        |\n",
      "|    time_elapsed    | 31        |\n",
      "|    total_timesteps | 24576     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26624, episode_reward=-19754.34 +/- 771.60\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.98e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025736955 |\n",
      "|    clip_fraction        | 0.0102       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.73e+05     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000522    |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 3.85e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=28672, episode_reward=-18875.10 +/- 682.57\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.89e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044280537 |\n",
      "|    clip_fraction        | 0.0179       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.81e+05     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 3.58e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30720, episode_reward=-19262.92 +/- 1231.38\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | -1.93e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003824004 |\n",
      "|    clip_fraction        | 0.014       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.85e+05    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.000611   |\n",
      "|    std                  | 0.993       |\n",
      "|    value_loss           | 3.86e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=32768, episode_reward=-18169.98 +/- 778.36\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.82e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025028167 |\n",
      "|    clip_fraction        | 0.00942      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.68e+05     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.000222    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.52e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.87e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 762       |\n",
      "|    iterations      | 16        |\n",
      "|    time_elapsed    | 42        |\n",
      "|    total_timesteps | 32768     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=34816, episode_reward=-19022.84 +/- 779.62\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.9e+04     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022370396 |\n",
      "|    clip_fraction        | 0.00684      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.62e+05     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000277    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.56e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=36864, episode_reward=-18293.55 +/- 304.08\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.83e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026765158 |\n",
      "|    clip_fraction        | 0.00889      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.07e+05     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00035     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.81e+05     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=38912, episode_reward=-19160.41 +/- 897.48\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.92e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012668844 |\n",
      "|    clip_fraction        | 0.0061       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.77e+05     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.000102    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.58e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40960, episode_reward=-18083.25 +/- 345.42\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.81e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056260834 |\n",
      "|    clip_fraction        | 0.0331       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.61e+05     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 3.5e+05      |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.86e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 710       |\n",
      "|    iterations      | 20        |\n",
      "|    time_elapsed    | 57        |\n",
      "|    total_timesteps | 40960     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=43008, episode_reward=-19096.30 +/- 1010.30\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.91e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045979703 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.75e+05     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000506    |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 3.71e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=45056, episode_reward=-18811.45 +/- 374.64\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | -1.88e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002469276 |\n",
      "|    clip_fraction        | 0.00498     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.66e+05    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.000548   |\n",
      "|    std                  | 0.97        |\n",
      "|    value_loss           | 3.54e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=47104, episode_reward=-18037.66 +/- 207.51\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.8e+04     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019677263 |\n",
      "|    clip_fraction        | 0.00376      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.58e+05     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.000106    |\n",
      "|    std                  | 0.962        |\n",
      "|    value_loss           | 3.44e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=49152, episode_reward=-19119.72 +/- 1072.09\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.91e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042552333 |\n",
      "|    clip_fraction        | 0.0174       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.94e+05     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    std                  | 0.961        |\n",
      "|    value_loss           | 3.62e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.86e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 658       |\n",
      "|    iterations      | 24        |\n",
      "|    time_elapsed    | 74        |\n",
      "|    total_timesteps | 49152     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=51200, episode_reward=-18187.86 +/- 347.81\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -1.82e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014882305 |\n",
      "|    clip_fraction        | 0.01         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.71e+05     |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.000322    |\n",
      "|    std                  | 0.954        |\n",
      "|    value_loss           | 3.55e+05     |\n",
      "------------------------------------------\n",
      "DDPG\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.83e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 186       |\n",
      "|    time_elapsed    | 10        |\n",
      "|    total_timesteps | 2000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 249       |\n",
      "|    critic_loss     | 212       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 1500      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2048, episode_reward=-18549.79 +/- 721.62\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.85e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 336       |\n",
      "|    critic_loss     | 282       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 2000      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | -1.8e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 136      |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 580      |\n",
      "|    critic_loss     | 623      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4096, episode_reward=-18385.28 +/- 659.90\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.84e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 4096      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 656       |\n",
      "|    critic_loss     | 706       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 4000      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.84e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 119       |\n",
      "|    time_elapsed    | 50        |\n",
      "|    total_timesteps | 6000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 871       |\n",
      "|    critic_loss     | 689       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 5500      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6144, episode_reward=-19260.14 +/- 766.78\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.93e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 6144      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 939       |\n",
      "|    critic_loss     | 719       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 6000      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.84e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 118       |\n",
      "|    time_elapsed    | 67        |\n",
      "|    total_timesteps | 8000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.14e+03  |\n",
      "|    critic_loss     | 990       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 7500      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8192, episode_reward=-18884.15 +/- 1059.63\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.89e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 8192      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.2e+03   |\n",
      "|    critic_loss     | 1.02e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 8000      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.85e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 114       |\n",
      "|    time_elapsed    | 87        |\n",
      "|    total_timesteps | 10000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.38e+03  |\n",
      "|    critic_loss     | 1.2e+03   |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 9500      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10240, episode_reward=-19909.25 +/- 742.35\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.99e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 10240     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.43e+03  |\n",
      "|    critic_loss     | 1.26e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 10000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.85e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 114       |\n",
      "|    time_elapsed    | 105       |\n",
      "|    total_timesteps | 12000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.6e+03   |\n",
      "|    critic_loss     | 1.36e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 11500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12288, episode_reward=-18994.03 +/- 696.56\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | -1.9e+04 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12288    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.66e+03 |\n",
      "|    critic_loss     | 1.37e+03 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.85e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 111       |\n",
      "|    time_elapsed    | 125       |\n",
      "|    total_timesteps | 14000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.81e+03  |\n",
      "|    critic_loss     | 1.48e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 13500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14336, episode_reward=-18192.55 +/- 617.14\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.82e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 14336     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.85e+03  |\n",
      "|    critic_loss     | 1.54e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 14000     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.86e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 112       |\n",
      "|    time_elapsed    | 142       |\n",
      "|    total_timesteps | 16000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.99e+03  |\n",
      "|    critic_loss     | 1.58e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 15500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16384, episode_reward=-18703.96 +/- 786.02\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.87e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 16384     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.04e+03  |\n",
      "|    critic_loss     | 1.71e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 16000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.87e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 36        |\n",
      "|    fps             | 113       |\n",
      "|    time_elapsed    | 158       |\n",
      "|    total_timesteps | 18000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.16e+03  |\n",
      "|    critic_loss     | 1.7e+03   |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 17500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18432, episode_reward=-18121.89 +/- 408.51\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.81e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 18432     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.2e+03   |\n",
      "|    critic_loss     | 1.78e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 18000     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.87e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 40        |\n",
      "|    fps             | 113       |\n",
      "|    time_elapsed    | 176       |\n",
      "|    total_timesteps | 20000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.31e+03  |\n",
      "|    critic_loss     | 1.85e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 19500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20480, episode_reward=-18600.78 +/- 659.87\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.86e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 20480     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.35e+03  |\n",
      "|    critic_loss     | 1.81e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 20000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.87e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 44        |\n",
      "|    fps             | 113       |\n",
      "|    time_elapsed    | 193       |\n",
      "|    total_timesteps | 22000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.45e+03  |\n",
      "|    critic_loss     | 2.01e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 21500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22528, episode_reward=-18584.35 +/- 208.28\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.86e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 22528     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.52e+03  |\n",
      "|    critic_loss     | 1.91e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 22500     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.87e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 48        |\n",
      "|    fps             | 111       |\n",
      "|    time_elapsed    | 215       |\n",
      "|    total_timesteps | 24000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.58e+03  |\n",
      "|    critic_loss     | 2.01e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 23500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24576, episode_reward=-18920.68 +/- 682.20\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.89e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 24576     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.64e+03  |\n",
      "|    critic_loss     | 2.1e+03   |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 24500     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.87e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 52        |\n",
      "|    fps             | 110       |\n",
      "|    time_elapsed    | 235       |\n",
      "|    total_timesteps | 26000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.7e+03   |\n",
      "|    critic_loss     | 2.16e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 25500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26624, episode_reward=-18946.34 +/- 898.76\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.89e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 26624     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.75e+03  |\n",
      "|    critic_loss     | 2.15e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 26500     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.87e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 56        |\n",
      "|    fps             | 111       |\n",
      "|    time_elapsed    | 252       |\n",
      "|    total_timesteps | 28000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.8e+03   |\n",
      "|    critic_loss     | 2.14e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 27500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=28672, episode_reward=-18740.64 +/- 435.25\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.87e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 28672     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.85e+03  |\n",
      "|    critic_loss     | 2.22e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 28500     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.87e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 60        |\n",
      "|    fps             | 110       |\n",
      "|    time_elapsed    | 270       |\n",
      "|    total_timesteps | 30000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.89e+03  |\n",
      "|    critic_loss     | 2.24e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 29500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30720, episode_reward=-19045.64 +/- 842.76\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | -1.9e+04 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30720    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.94e+03 |\n",
      "|    critic_loss     | 2.13e+03 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 30500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.87e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 64        |\n",
      "|    fps             | 110       |\n",
      "|    time_elapsed    | 289       |\n",
      "|    total_timesteps | 32000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.98e+03  |\n",
      "|    critic_loss     | 2.2e+03   |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 31500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=32768, episode_reward=-18915.24 +/- 535.88\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.89e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 32768     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.01e+03  |\n",
      "|    critic_loss     | 2.32e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 32500     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.88e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 68        |\n",
      "|    fps             | 110       |\n",
      "|    time_elapsed    | 308       |\n",
      "|    total_timesteps | 34000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.05e+03  |\n",
      "|    critic_loss     | 2.28e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 33500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=34816, episode_reward=-18452.04 +/- 688.69\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.85e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 34816     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.08e+03  |\n",
      "|    critic_loss     | 2.42e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 34500     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.88e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 72        |\n",
      "|    fps             | 110       |\n",
      "|    time_elapsed    | 325       |\n",
      "|    total_timesteps | 36000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.12e+03  |\n",
      "|    critic_loss     | 2.4e+03   |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 35500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=36864, episode_reward=-18754.39 +/- 889.05\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.88e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 36864     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.15e+03  |\n",
      "|    critic_loss     | 2.3e+03   |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 36500     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.87e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 76        |\n",
      "|    fps             | 110       |\n",
      "|    time_elapsed    | 343       |\n",
      "|    total_timesteps | 38000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.18e+03  |\n",
      "|    critic_loss     | 2.59e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 37500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=38912, episode_reward=-18376.82 +/- 751.51\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.84e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 38912     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.2e+03   |\n",
      "|    critic_loss     | 2.47e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 38500     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.88e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 80        |\n",
      "|    fps             | 110       |\n",
      "|    time_elapsed    | 361       |\n",
      "|    total_timesteps | 40000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.23e+03  |\n",
      "|    critic_loss     | 2.61e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 39500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40960, episode_reward=-18757.85 +/- 728.04\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.88e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 40960     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.25e+03  |\n",
      "|    critic_loss     | 2.38e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 40500     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.88e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 84        |\n",
      "|    fps             | 110       |\n",
      "|    time_elapsed    | 378       |\n",
      "|    total_timesteps | 42000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.28e+03  |\n",
      "|    critic_loss     | 2.48e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 41500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=43008, episode_reward=-19111.26 +/- 685.35\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.91e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 43008     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.31e+03  |\n",
      "|    critic_loss     | 2.71e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 43000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.88e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 88        |\n",
      "|    fps             | 110       |\n",
      "|    time_elapsed    | 396       |\n",
      "|    total_timesteps | 44000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.32e+03  |\n",
      "|    critic_loss     | 2.54e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 43500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=45056, episode_reward=-19041.57 +/- 462.05\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | -1.9e+04 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45056    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.35e+03 |\n",
      "|    critic_loss     | 2.6e+03  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 45000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.88e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 92        |\n",
      "|    fps             | 110       |\n",
      "|    time_elapsed    | 416       |\n",
      "|    total_timesteps | 46000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.36e+03  |\n",
      "|    critic_loss     | 2.68e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 45500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=47104, episode_reward=-18956.43 +/- 363.48\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | -1.9e+04 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47104    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.38e+03 |\n",
      "|    critic_loss     | 2.71e+03 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 47000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.89e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 96        |\n",
      "|    fps             | 110       |\n",
      "|    time_elapsed    | 435       |\n",
      "|    total_timesteps | 48000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.39e+03  |\n",
      "|    critic_loss     | 2.64e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 47500     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=49152, episode_reward=-18899.36 +/- 300.14\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -1.89e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 49152     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.42e+03  |\n",
      "|    critic_loss     | 2.31e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 49000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.88e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 100       |\n",
      "|    fps             | 109       |\n",
      "|    time_elapsed    | 455       |\n",
      "|    total_timesteps | 50000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.43e+03  |\n",
      "|    critic_loss     | 2.53e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 49500     |\n",
      "----------------------------------\n",
      "SAC\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -8.38e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 66        |\n",
      "|    time_elapsed    | 30        |\n",
      "|    total_timesteps | 2000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 179       |\n",
      "|    critic_loss     | 1.04e+03  |\n",
      "|    ent_coef        | 1.9       |\n",
      "|    ent_coef_loss   | -4.81     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1899      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=-5260.98 +/- 946.59\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -5.26e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 190       |\n",
      "|    critic_loss     | 1.69e+03  |\n",
      "|    ent_coef        | 1.93      |\n",
      "|    ent_coef_loss   | -4.79     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1947      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -7.07e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 62        |\n",
      "|    time_elapsed    | 63        |\n",
      "|    total_timesteps | 4000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 392       |\n",
      "|    critic_loss     | 1.69e+03  |\n",
      "|    ent_coef        | 3.59      |\n",
      "|    ent_coef_loss   | -6.64     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3899      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4096, episode_reward=-5092.30 +/- 1152.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -5.09e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 4096      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 393       |\n",
      "|    critic_loss     | 1.6e+03   |\n",
      "|    ent_coef        | 3.69      |\n",
      "|    ent_coef_loss   | -7.93     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3995      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -6.87e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 57        |\n",
      "|    time_elapsed    | 104       |\n",
      "|    total_timesteps | 6000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 694       |\n",
      "|    critic_loss     | 2.99e+03  |\n",
      "|    ent_coef        | 6.24      |\n",
      "|    ent_coef_loss   | -8.78     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 5899      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6144, episode_reward=-5631.76 +/- 774.06\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -5.63e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 6144      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 714       |\n",
      "|    critic_loss     | 3.85e+03  |\n",
      "|    ent_coef        | 6.49      |\n",
      "|    ent_coef_loss   | -9.37     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 6043      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -6.92e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 55        |\n",
      "|    time_elapsed    | 144       |\n",
      "|    total_timesteps | 8000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.18e+03  |\n",
      "|    critic_loss     | 1.49e+04  |\n",
      "|    ent_coef        | 10.9      |\n",
      "|    ent_coef_loss   | -10.2     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 7899      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8192, episode_reward=-4837.72 +/- 1017.69\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -4.84e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 8192      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.26e+03  |\n",
      "|    critic_loss     | 2.15e+04  |\n",
      "|    ent_coef        | 11.5      |\n",
      "|    ent_coef_loss   | -12.8     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 8091      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -7.17e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 55        |\n",
      "|    time_elapsed    | 179       |\n",
      "|    total_timesteps | 10000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.99e+03  |\n",
      "|    critic_loss     | 2.37e+04  |\n",
      "|    ent_coef        | 19.2      |\n",
      "|    ent_coef_loss   | -12       |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 9899      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10240, episode_reward=-7465.68 +/- 4080.47\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -7.47e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 10240     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.18e+03  |\n",
      "|    critic_loss     | 1.92e+04  |\n",
      "|    ent_coef        | 20.6      |\n",
      "|    ent_coef_loss   | -14.1     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 10139     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -7.52e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 56        |\n",
      "|    time_elapsed    | 212       |\n",
      "|    total_timesteps | 12000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.48e+03  |\n",
      "|    critic_loss     | 1.3e+05   |\n",
      "|    ent_coef        | 34.3      |\n",
      "|    ent_coef_loss   | -14.6     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 11899     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12288, episode_reward=-5269.28 +/- 1117.59\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -5.27e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 12288     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.97e+03  |\n",
      "|    critic_loss     | 2.41e+04  |\n",
      "|    ent_coef        | 34.4      |\n",
      "|    ent_coef_loss   | 4.31      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 12187     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -7.09e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 56        |\n",
      "|    time_elapsed    | 247       |\n",
      "|    total_timesteps | 14000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.77e+03  |\n",
      "|    critic_loss     | 1.26e+04  |\n",
      "|    ent_coef        | 26.7      |\n",
      "|    ent_coef_loss   | 4.37      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 13899     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14336, episode_reward=-4446.43 +/- 933.29\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -4.45e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 14336     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.68e+03  |\n",
      "|    critic_loss     | 9.2e+03   |\n",
      "|    ent_coef        | 24.9      |\n",
      "|    ent_coef_loss   | 4.45      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 14235     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | -6.7e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 58       |\n",
      "|    time_elapsed    | 273      |\n",
      "|    total_timesteps | 16000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.55e+03 |\n",
      "|    critic_loss     | 4.75e+03 |\n",
      "|    ent_coef        | 16.4     |\n",
      "|    ent_coef_loss   | 3.72     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 15899    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=16384, episode_reward=-5139.84 +/- 931.98\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -5.14e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 16384     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.52e+03  |\n",
      "|    critic_loss     | 3.56e+03  |\n",
      "|    ent_coef        | 14.7      |\n",
      "|    ent_coef_loss   | 3.59      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 16283     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -6.57e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 36        |\n",
      "|    fps             | 59        |\n",
      "|    time_elapsed    | 301       |\n",
      "|    total_timesteps | 18000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.45e+03  |\n",
      "|    critic_loss     | 3.85e+03  |\n",
      "|    ent_coef        | 9.23      |\n",
      "|    ent_coef_loss   | 2.59      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 17899     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18432, episode_reward=-4274.36 +/- 362.97\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -4.27e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 18432     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.4e+03   |\n",
      "|    critic_loss     | 1.36e+03  |\n",
      "|    ent_coef        | 8.13      |\n",
      "|    ent_coef_loss   | 2.82      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 18331     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -6.46e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 40        |\n",
      "|    fps             | 61        |\n",
      "|    time_elapsed    | 326       |\n",
      "|    total_timesteps | 20000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.33e+03  |\n",
      "|    critic_loss     | 1.5e+03   |\n",
      "|    ent_coef        | 5.14      |\n",
      "|    ent_coef_loss   | 1.85      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 19899     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20480, episode_reward=-4647.02 +/- 570.50\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -4.65e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 20480     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.27e+03  |\n",
      "|    critic_loss     | 834       |\n",
      "|    ent_coef        | 4.48      |\n",
      "|    ent_coef_loss   | 1.73      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 20379     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -6.29e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 44        |\n",
      "|    fps             | 62        |\n",
      "|    time_elapsed    | 351       |\n",
      "|    total_timesteps | 22000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.23e+03  |\n",
      "|    critic_loss     | 900       |\n",
      "|    ent_coef        | 3.01      |\n",
      "|    ent_coef_loss   | 0.846     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 21899     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22528, episode_reward=-5218.29 +/- 649.48\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -5.22e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 22528     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.22e+03  |\n",
      "|    critic_loss     | 571       |\n",
      "|    ent_coef        | 2.64      |\n",
      "|    ent_coef_loss   | 0.786     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 22427     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -6.17e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 48        |\n",
      "|    fps             | 63        |\n",
      "|    time_elapsed    | 378       |\n",
      "|    total_timesteps | 24000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.16e+03  |\n",
      "|    critic_loss     | 624       |\n",
      "|    ent_coef        | 1.94      |\n",
      "|    ent_coef_loss   | 0.303     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 23899     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24576, episode_reward=-5744.80 +/- 590.05\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -5.74e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 24576     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.14e+03  |\n",
      "|    critic_loss     | 446       |\n",
      "|    ent_coef        | 1.77      |\n",
      "|    ent_coef_loss   | 0.159     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 24475     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | -6.1e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 64       |\n",
      "|    time_elapsed    | 403      |\n",
      "|    total_timesteps | 26000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.08e+03 |\n",
      "|    critic_loss     | 560      |\n",
      "|    ent_coef        | 1.51     |\n",
      "|    ent_coef_loss   | 0.0917   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 25899    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26624, episode_reward=-6125.96 +/- 561.70\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -6.13e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 26624     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.05e+03  |\n",
      "|    critic_loss     | 594       |\n",
      "|    ent_coef        | 1.47      |\n",
      "|    ent_coef_loss   | 0.0391    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 26523     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -6.01e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 56        |\n",
      "|    fps             | 65        |\n",
      "|    time_elapsed    | 429       |\n",
      "|    total_timesteps | 28000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.99e+03  |\n",
      "|    critic_loss     | 722       |\n",
      "|    ent_coef        | 1.42      |\n",
      "|    ent_coef_loss   | 0.0494    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 27899     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=28672, episode_reward=-4658.61 +/- 843.59\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -4.66e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 28672     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.98e+03  |\n",
      "|    critic_loss     | 844       |\n",
      "|    ent_coef        | 1.4       |\n",
      "|    ent_coef_loss   | 0.0364    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 28571     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -5.95e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 60        |\n",
      "|    fps             | 64        |\n",
      "|    time_elapsed    | 468       |\n",
      "|    total_timesteps | 30000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.94e+03  |\n",
      "|    critic_loss     | 1.05e+03  |\n",
      "|    ent_coef        | 1.27      |\n",
      "|    ent_coef_loss   | 0.0124    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 29899     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=30720, episode_reward=-4895.52 +/- 1106.08\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | -4.9e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30720    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.9e+03  |\n",
      "|    critic_loss     | 1.03e+03 |\n",
      "|    ent_coef        | 1.25     |\n",
      "|    ent_coef_loss   | -0.022   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 30619    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -5.94e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 64        |\n",
      "|    fps             | 62        |\n",
      "|    time_elapsed    | 510       |\n",
      "|    total_timesteps | 32000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.87e+03  |\n",
      "|    critic_loss     | 1.26e+03  |\n",
      "|    ent_coef        | 1.27      |\n",
      "|    ent_coef_loss   | 0.0187    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 31899     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=32768, episode_reward=-4597.85 +/- 583.90\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | -4.6e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32768    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.85e+03 |\n",
      "|    critic_loss     | 1.1e+03  |\n",
      "|    ent_coef        | 1.28     |\n",
      "|    ent_coef_loss   | 0.0167   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 32667    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | -5.9e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 61       |\n",
      "|    time_elapsed    | 555      |\n",
      "|    total_timesteps | 34000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.8e+03  |\n",
      "|    critic_loss     | 1.2e+03  |\n",
      "|    ent_coef        | 1.28     |\n",
      "|    ent_coef_loss   | 0.00785  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 33899    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34816, episode_reward=-4468.18 +/- 1080.07\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -4.47e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 34816     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.78e+03  |\n",
      "|    critic_loss     | 1.24e+03  |\n",
      "|    ent_coef        | 1.29      |\n",
      "|    ent_coef_loss   | -0.0122   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 34715     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -5.89e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 72        |\n",
      "|    fps             | 61        |\n",
      "|    time_elapsed    | 581       |\n",
      "|    total_timesteps | 36000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.73e+03  |\n",
      "|    critic_loss     | 1.05e+03  |\n",
      "|    ent_coef        | 1.28      |\n",
      "|    ent_coef_loss   | 0.00174   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 35899     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=36864, episode_reward=-4827.60 +/- 730.09\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -4.83e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 36864     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.73e+03  |\n",
      "|    critic_loss     | 1.14e+03  |\n",
      "|    ent_coef        | 1.31      |\n",
      "|    ent_coef_loss   | 0.0611    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 36763     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -5.84e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 76        |\n",
      "|    fps             | 62        |\n",
      "|    time_elapsed    | 605       |\n",
      "|    total_timesteps | 38000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.69e+03  |\n",
      "|    critic_loss     | 1.34e+03  |\n",
      "|    ent_coef        | 1.34      |\n",
      "|    ent_coef_loss   | -0.0156   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 37899     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=38912, episode_reward=-5085.20 +/- 978.99\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -5.09e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 38912     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.68e+03  |\n",
      "|    critic_loss     | 1.42e+03  |\n",
      "|    ent_coef        | 1.35      |\n",
      "|    ent_coef_loss   | -0.0587   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 38811     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -5.77e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 80        |\n",
      "|    fps             | 63        |\n",
      "|    time_elapsed    | 631       |\n",
      "|    total_timesteps | 40000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.66e+03  |\n",
      "|    critic_loss     | 1.62e+03  |\n",
      "|    ent_coef        | 1.33      |\n",
      "|    ent_coef_loss   | -0.015    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 39899     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40960, episode_reward=-4762.75 +/- 978.52\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -4.76e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 40960     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.64e+03  |\n",
      "|    critic_loss     | 1.18e+03  |\n",
      "|    ent_coef        | 1.37      |\n",
      "|    ent_coef_loss   | -0.00607  |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 40859     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -5.72e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 84        |\n",
      "|    fps             | 64        |\n",
      "|    time_elapsed    | 655       |\n",
      "|    total_timesteps | 42000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.64e+03  |\n",
      "|    critic_loss     | 1.28e+03  |\n",
      "|    ent_coef        | 1.35      |\n",
      "|    ent_coef_loss   | 0.0484    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 41899     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=43008, episode_reward=-4657.86 +/- 755.26\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -4.66e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 43008     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.61e+03  |\n",
      "|    critic_loss     | 1.57e+03  |\n",
      "|    ent_coef        | 1.35      |\n",
      "|    ent_coef_loss   | 0.0822    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 42907     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -5.72e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 88        |\n",
      "|    fps             | 63        |\n",
      "|    time_elapsed    | 688       |\n",
      "|    total_timesteps | 44000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.59e+03  |\n",
      "|    critic_loss     | 1.49e+03  |\n",
      "|    ent_coef        | 1.4       |\n",
      "|    ent_coef_loss   | 0.0409    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 43899     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=45056, episode_reward=-4715.20 +/- 852.86\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -4.72e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 45056     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.57e+03  |\n",
      "|    critic_loss     | 1.88e+03  |\n",
      "|    ent_coef        | 1.37      |\n",
      "|    ent_coef_loss   | -0.0345   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 44955     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -5.68e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 92        |\n",
      "|    fps             | 64        |\n",
      "|    time_elapsed    | 716       |\n",
      "|    total_timesteps | 46000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.54e+03  |\n",
      "|    critic_loss     | 1.83e+03  |\n",
      "|    ent_coef        | 1.34      |\n",
      "|    ent_coef_loss   | -0.0338   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 45899     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=47104, episode_reward=-4967.16 +/- 612.13\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -4.97e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 47104     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.55e+03  |\n",
      "|    critic_loss     | 1.85e+03  |\n",
      "|    ent_coef        | 1.38      |\n",
      "|    ent_coef_loss   | -0.0167   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 47003     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -5.62e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 96        |\n",
      "|    fps             | 64        |\n",
      "|    time_elapsed    | 744       |\n",
      "|    total_timesteps | 48000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.52e+03  |\n",
      "|    critic_loss     | 2.04e+03  |\n",
      "|    ent_coef        | 1.37      |\n",
      "|    ent_coef_loss   | 0.0236    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 47899     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=49152, episode_reward=-5286.90 +/- 1085.05\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 500       |\n",
      "|    mean_reward     | -5.29e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 49152     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.52e+03  |\n",
      "|    critic_loss     | 2.12e+03  |\n",
      "|    ent_coef        | 1.42      |\n",
      "|    ent_coef_loss   | -0.0419   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 49051     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -5.56e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 100       |\n",
      "|    fps             | 64        |\n",
      "|    time_elapsed    | 770       |\n",
      "|    total_timesteps | 50000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.51e+03  |\n",
      "|    critic_loss     | 1.79e+03  |\n",
      "|    ent_coef        | 1.44      |\n",
      "|    ent_coef_loss   | -0.0279   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 49899     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x7f45e3ffe640>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, DDPG, SAC\n",
    "\n",
    "\n",
    "# Separate evaluation env\n",
    "eval_env = gym.make('Dam-v1')\n",
    "\n",
    "ppo = PPO(\"MlpPolicy\", env, verbose=1, policy_kwargs=dict(net_arch=[64, 64]))\n",
    "ddpg = DDPG(\"MlpPolicy\", env, verbose=1, policy_kwargs=dict(net_arch=[64, 64]))\n",
    "sac = SAC(\"MlpPolicy\", env, verbose=1, policy_kwargs=dict(net_arch=[64, 64]))\n",
    "\n",
    "print('PPO')\n",
    "ppo.learn(total_timesteps=50000, eval_env=eval_env, \n",
    "          eval_log_path='./logs/dam/ppo', eval_freq=2048, log_interval=4)\n",
    "\n",
    "print('DDPG')\n",
    "ddpg.learn(total_timesteps=50000, eval_env=eval_env, \n",
    "          eval_log_path='./logs/dam/ddpg', eval_freq=2048, log_interval=4)\n",
    "\n",
    "print('SAC')\n",
    "sac.learn(total_timesteps=50000, eval_env=eval_env, \n",
    "          eval_log_path='./logs/dam/sac', eval_freq=2048, log_interval=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEGCAYAAABcolNbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAABsh0lEQVR4nO2dd5gc1ZW331Od0+SgCRqNMkgEASIKMDnYGLxgbBzxOrBe1hEn/LEOgLNZr7MxTgu2MdjYJGOMyTkKCZRQTiNpcuiZzl11vz+qZtSTe5JmkO47Tz1dfSvd6umu3z3nnnuuKKXQaDQajWaqMKa7AhqNRqM5uNFCo9FoNJopRQuNRqPRaKYULTQajUajmVK00Gg0Go1mSnFPdwVmGmVlZaq+vn66q6HRaDRvKlauXNmqlCofapsWmgHU19fzyiuvTHc1NBqN5k2FiOwcbpt2nWk0Go1mStFCo9FoNJopRQuNRqPRaKYULTQajUajmVK00Gg0Go1mStFCo9FoNJopRQuNRqPRaKYULTQajUajmVK00Gj6kTEzNMebaY43o+cq0kwFsUyMxlgjpmVOd1U0BwidGUCDUopoOkpnqpOeTE9feU+6h5pIDT6Xb0qumzbTeF3eKTm3ZuaRNtM0xZqIZqIAdKW6mBWaRaGvcJprpplqtNAcwqTMFB3JDrpSXWRVdtD2hJlgW+c2KoIVlAZKJ/W6TbEmujPdFHgLmBWahcfwTNr5NTML0zJpSbTQnmxHsd9KzqosDT0NRFNRZoX1d+BgRgvNIYalLKKpKB2pDuLZ+Oj7Y9EYb6Q73U1NuAaPa/wPg6EeONF0lFgmRkWwghJ/ybjPrZl5KKVoT7bTmmgdsiHTSzQTJdapvwMHM1poDhGS2aRtvaS7MNXYfeOxbIytXVupClWN2dWhlKIj1UFLvGXIB46pTPbF9tGZ6qQ6VI3f7R9z/TTjQymFpSyU84cCl+HCkIl130bTUZpjzaSsVF77934HulJdVIerp8xdOxppM41LXLgM17Rc/2BFC81BjmmZNPQ09Ot7Gfe5lH2uaDpKVagKtzH616e34zdpJkfdN5FNsK1rG6X+UsqD5RN+2B3qWMpiR3QHpmViKQvAlhOl9gvLEAiCz+XD7/YTcAfwu+xXERn1mslsksZYI7FsbFx1jmfjbO3cSnmgnLJAWV7XnAhZK0ssE6M73U0sE+trCBkYuA03HsPT/9XlwS37X4ern6UsW8Sx+sS8T9CVIugJHlLfby0000gim8AlrinrEM9aWXZGd+b1kB8L0XSUeCZOTbiGsDc85D4DO37zRaFoTbb2idlw5x+NjJUhlo4Ry8ZQSuFz+fC6vHhdXnwu3yHxI2+ON5PIJsZ8nEKRNJMkzSSdqU7AFh+/y79ffNx+/C5/34M2Y2VoibfQkeqYcL0ViuZEc993IOgJTvicuSSyCXrSPXRnuof9fCws0laatJUe9jyC2NaPuPoLCtaodXCLm2J/McX+4kOib0p0CGt/li9frqZyPppENkE0FSWajpK20rjFTW2klpAnNKnXyZgZdkZ35u26GC8lvhIqQ5V9D25LWbTEW2hLtg3bYh4L+QYLWMoilokRy8ToSfeMet9ucQ8SH6/Li9fwTnkr+kAQz8TZHt0+pdfoFR+vy0t3ujuvB+x4KPWXUhGsGHfjwFIWPZkeW1zS3SP2Fx1oBKHQV0ipv/RN7zIWkZVKqeVDbdMWzQEgmU0STUeJpqKDHoBZZVsds0KzJq0jNGWm2BndScbKTMr5RqI91U5PpoeacI1txcSbJvWH3BssUBmspNhf3FeulCKRTdjCkukhkU2MSdiyKks2mx3k4ul1GxX5iij2F78pLR+lFPti+6b+OigSZoKEOXaraSy0JduIpqOEPWEEuxEwVGOgd1vu9ngmTjwbn5RGz1SgUHSmOulMdRL2hCnxlxDxRqa7WpOOFpopImWm6Ep1DSkuA1HYD4ZUNsWs0KwJtagT2QS7orsOaKstbaWntPVsKpO9sb10pjop8BYQz8SJZWN5BTXs7dmLQlETrsnrWr1uo8Z4Iy2JFop9xZT4SyYUbXegaUu2Tbq7dLrJWJlJccvNZHoyPfRkevAZPkoDpRT6Csfc0LGURcpMkTbTRLyRGdNQ0kIziWTMDJ2pTqLp6Lh+6O2pdlJmitpIbV4d7QOJZ+Ls6t41rqiyNwPxbDyvkGywW/X3b72fP73xJxSK2ZHZnFR1EidXn0x1uDqvc5jKpDXZSluyjQJvAaWBUgLuwERuYcpJm2la4i3TXQ3NBEhZKfbG9tIcbx6yHydrZUmbaTJWhrRp9yOlTXvJbWAuKl40Y4RG99EMYCJ9NPt69tGeap9wHbyGl7qCujGFePake9jdvXvK/OSTTdbKsrZ1LUtLl066tRDPxLn5tZt5qfElTqo6icNKDuP5fc+zsX0jAHMK5vSJzqzQrDGdO+gOUhoopcBbMKl1nix2RndOSoThVJIxM2SszKR38h+sCELIE8K0TNJWOu+G5KLiRQc00GCkPhotNAOYCUID4BIXteHavKKuulJd7OnZM2P90ANpjjfzk1d/wubOzVSFqvjokR9ladnSSTl3Q3cD//PK/9AUb+J9h7+Pt859a58rsj3Rzgv7XuCFfS+wqWMTAPUF9ZxcfTInVZ1EZagy7+t4DS8l/pIZ1Y/TmexkT2zPtNZBKUV3upvWRGvf0pZs27+eaKMr1QXAwuKFnFh1IifMOoHyYPm01vtgRAvNDGamCA3YLZmKYAVlgbJh9+lIdrAvtu9NIzIvN77Mza/djKUsLp5/MY/vfpzmeDNvqX0L71vyvglZCs/vfZ6bX7sZv9vPZ479DIeXHj7svq2JVl7c9yIv7H2BzZ2bAZhXOI+Tq0/mxKoTqQhW5HVNl7jsfpxAybSGqWatLFs7tx7QvrloKsqa1jVsaNtAU7ypT0wGBqF4DS9lgTJKA6WUBcooC5RhKYtXml5hZ3QnYH/2vaJTFa46YPdwMKOFZgYzk4Sml2JfMVWhqkFBAq2JVpriTZN+vakgY2b444Y/8s8d/2Re4Tw+deynmBWaRcpM8bfNf+PvW/9O0B3k/Uvez+m1p48pICJrZbl9w+38Y/s/WFS8iM8c95m+CL7eH9pIEXgt8Rbb0tn7Alu7tgJwYtWJXL7ocmojtXnVwcCg2F9MWaBsXP1rE2VPz56+MS+9PLbrMe7efDdzCuawoHgBC4oWMK9w3rhdVmkzzYb2DaxtWcua1jXsiO4AIOQJUR2q7ickveulgVIinsiw/8/GWCMv7XuJFxtfZGun/dnXReo4oeoETpx1IrWR2mkLN0+ZKTqTdkRYV6qLzlQnHakOupJdfZFinalOsla23/iioDtov3cF+sp7t/Wu10ZqKQ+UT+m9aaGZwcxEoQG7b2B2ZHbfQ6w53kxL4s3R6dsYa+RHr/6I7V3buXDuhbz3sPcO6pfZHd3Nr9b8ik0dm1haupSPHPmRvDrtO5Od/OjVH7GhfQMX1F/A+5e8v+8zMjCoL6zH7/LnlXML7M/18d2P8+C2B0mZKVbUrOCdi96Zd1+OS1yU+kspDZQeMJdaT7qHnd07+5Vt69zGV5/7KpXBSkxl0hhrBGwruSZcw4LiBcwvms+CogXUReqGTLliKYsdXTtY07qGNa1r2Ni+kYyVwSUuFpcs5siyIzmq/CjmFs6dlHttTbTy0r6XeKnxJTa2b0ShqApV9YnO3MK5U/JgtpTF5o7NrG5ezcaOjXQkO+hMdQ45mLN33EuRr4hCXyHF/mLchptkNkkim+h7zV0fLjCo2FfMwuKFLC5ZzKLiRcwtnDupjRQtNCMgIl8HPgb0PkX/n1LqH862LwMfAUzgU0qph5zyC4AfAS7g10qp7zjlc4E7gFJgJfABpdTwQ32ZuUIDduu8LlJHZ6qTtmTbqPsrpXh6z9M8uutRPnnMJ0d0wU0Vz+19jl+9/isMMfjPo/+T5bOG/B4C9g/+sV2PcfuG20lbaf5twb9x8fyLhw0W2Ni+kR+u/CHxbJyPHfUxTq05td/26lB1v7E3lrJoS7TRlmwbtUM1mo5y/9b7eWj7Q2RVljNqz+DSRZfm/Rm6xU1ZoIwSf8mUtlotZbG1c2u/EeyxTIwvP/1lTMvkO6d/h4g3Qk+6h62dW9nSucVeOrbQnekGbNfW3MK5fVZPIpvg9ZbXWde6rm+fukgdR5YfyRFlR3B4yeFTPriwM9nJy00v8+K+F1nfth5LWRT5ilhcspjFxYtZXLKY+oL6ceck605381rLa6xqWsVrLa/Rk+nBEIN5hfMoC5T1jaPqFZUiXxFF/iIKvAXjCjlOZpO28Jj22K8dXTvY1LGJTR2baI43A/bve17RPBYVL2JxsS0+Bb7xu5K10IyAIzQ9SqmbBpQvAf4EnABUA48Ai5zNm4BzgQbgZeA9Sqn1IvJn4G9KqTtE5GbgNaXUL0a6/kwWmrEQz8T59Zpf89ze5wC4aN5FvH/J+w/Y9dNmmlvX3cqjux5lYfFCPnXMp/Lu8O1MdnLb+tt4bu9zVIer+diRH+vX36KU4qEdD/H79b+nPFDONcuvoa6grt85in3Fw1pEpmXSmmilPdk+apReZ7KTe7bcwyO7HgHg7LqzeceCd/QTsJHwGl7KA+UU+Yvy2n+sNMWaaE229r1XSvGDlT/g1aZX+dopX2NR8aIhj1NK0Rxv7ic8O6I7+lyMxb5ijiw/kiPL7GWq6p8P3eluVjatZG3rWt5of4PWhH2/PpePBUULbPEpWczCooXDugWVUuyM7mRV8ypWNa9ic8dmFIoCbwHLKpaxrGIZR5UdNe6URxOhI9nB5o7NbOzYyKb2TWzr2tbXEJoVmtUnPEdXHD2mxqIWmhEYQWi+DKCU+rbz/iHg687mryulzs/dD/gOtlU0SymVFZGTc/cbjoNBaDa2b+Snq35KW7KNyxZexo7oDta3refn5/z8gGTF3dO9hx+9+iN2de/i4vkX867F7xqXS2BV8yp+u+a3tCRaOGP2Gbzv8PfhMTz86vVf8ezeZzmu8jiuXnb1oPQ9AVeA+sL6UVueGStDW6Jt0DwpQ9GaaOXuzXfzxO4ncImL8+rP4+IFF+cdvOB3+SkPlk9qWHQym2Rb17Z+df/Htn9w2/rb+MCSD/C2eW8b0/myVpZd0V14XV5qwjUzNhVPW6Kt76H8Rvsb7IzuRKEQhDkFc1hUvIjDSg5jXtE8dkV3sbp5NauaV/UN+JxXOI9jKo7hmIpjmFc0b8ZEDfaSNtNs79rOxvaNfVZPNG3nDJwdmW0LY/kyFpcsHvF3pYVmBByh+RAQBV4BPqeU6hCRnwIvKKX+4Oz3G+BB57ALlFIfdco/AJyILUIvKKUWOOWzgQeVUkcMcc2rgKsA6urqjtu5c+fAXfJiOKHpddnsje1lX88+9vbsZV/Mfg16grx9/ttZUb1iwqnJTcvk7i1389dNf6U8WM4njvkEi4oXsb5tPTc8fwP/cdR/cGbdmRO6xmg8uftJfrv2t3hdXv5r2X+xrGLZhM6XMlPctekuHtj2ACFPiAJvAXt79vKuxe/ikgWXDHpIuMTFvMJ5Y0pUmjEzNCea6Up1jSo4jbFG/rrprzyz5xl8Lh8XzruQi+ZdlHeuuoA7QGWwclJy223r2tavH2Fzx2a+/tzXOabiGD63/HMzVigmm0Q2YVsE7RvZ2LGRzR2bSZn7s3EE3AGOKj+KYyqOYVn5smm1zsaDUoq9PXtZ3WIL5oa2DZjKJOAOcGTZkfZ9VSwbZGUf8kIjIo8AQ/WuXge8ALQCCrgRqFJKfXgqhSaXiVg0Wzq2sL59PXt79vYTlcZYYz8fesAdoDpUTVW4it3du9kZ3UlFsIJL5l/CW2a/ZVyt/+Z4Mz9b9TM2dmzktJrT+Pcj/r3PjaCU4otPfRGXuPj2ad+ekgdQMpvkt2t/y1MNT3F4yeF88thPTuokVjujO/n167+mMd7IJ475BEeXHz3kfnWRunHnikqZKZrjzX2tx5Fo6G7grk138cK+Fwh5Qrxt3ts4v/78vAUk7AlT6i8l5AmN6//RlmijMd7Y974n3cO1T1+LIHz7tG8fUBeQS1wzKhuFaZnsjO5kW9c2qkJVo7b832wksgnWtq5lVfMqVjevpj1pN27rC+r7RGdh8UIOKzns0BaafBGReuDvSqkjZrrr7IFtD3Dt09f2vTfEoCJQQVW4iupwNVUh+7U6VE2hr7Dv4aKUYmXTSv62+W99c7FcvOBizpx9Zt6t8mf3PMuv1/wagI8c8RFOrT110D6P7HyEX6/5Ndefcj2LSxaP+f5GwrRMvvLsV9jetZ1LF17KpQsvnZKJo5RSmMoc9qFR5i8b06DL4UhkEzTGGvNKd7Ojawd/2fQXVjatJOAOcN6c87hw3oUU+YryupZb3H0dzvl2sGfMDFs6t/T1L1nK4qaXb+K1lte4YcUNzC+an9d5JoMCbwHVoWosrL6s5PmmCdJMHKUUu7r3uwc3dWzCUhZhT5gzZp/Bt0791gGzbN9UQiMiVUqpfc76Z4ETlVJXiMhS4Hb2BwM8CiwEBDsY4GxgD3YwwHuVUutE5C/AX3OCAV5XSv18pOuPV2h2Rndy9+a7KfIXUR2qpjJUOaZWlFKK11te52+b/8bGjo0U+Yp4+/y3c3bd2cM+gBLZBL9b+zueaniKhcUL+eQxnxx2oGEym+TqR65mWcUyPnXsp8Z8fyPxVMNT/Hz1z7l62dWcXnv6pJ47X0LuEHMK5kzqj6oz2Zl3NurtXdu5d8u9vLjvRdyGm7PqzuKieReNacR7wBWg0FdIoa9wxO/OruiuvmgwgPu33s8fN/yRDy39EBfMvSDv600EA4NZoVlDBkVkrAzd6W6iqei4J0DTjI+edA9rWtewunk1frefm95y0+gHTRJvNqH5PbAM23W2A/iPHOG5DvgwkAU+o5R60Cl/K/BD7PDm3yqlvumUz8MOby4BVgHvV0qNmEp5uoMBlFKsb1vP3zb/jXVt64h4I7xt3ts4b855/SJqNnds5ierfkJLvCVvK+LWdbfyrx3/4qdn/zTvqKnRMC2Tzz3xOXxuH98+7dt5d6x6DA/VoWrak+39HprjwS1u5hfNnxL3yFjn19nbs5f7t97PUw1PAbCiZgWXzL+Emkh+2aPBHqsR9oQp9BVS4C3oJ55dqS4aehr63m9s38j1z1/P8bOO5zPHfuaAtF59ho/aSG1eFliu6MzkdP3jwS1uwt4wbnGTttJkzMyYcpFNNYd8H81MZrqFJpeN7Ru5e/PdrG5ZTcgT4sK5F3J+/fk8svMR/rLpL5T4S/jEMZ/gsJLD8q7fZ5/4LJcvupzLFl02KXV8cveT/OK1X/C55Z/j+FnH53WMz/Axp2BO3/iYaDrKvp5940qfIgj1BfVTnqAxZaZoijXlLYqtiVYe2PYAj+58lIyV4fhZx3PJgkvG7NZyi5sCbwGFvkJ8Lh9bOrf0fU7RVJRrn74Wj+Hh26d9e9BnIAgF3oJJnZSsyFdEVahqXJFaWStLNB3tmzb5zSg6AVeAsDdM2BMe9juXm125N2V/rxAdyBRBWmhmMDNJaHrZ2rmVezbfw8tNLyMICsXJ1Sfz0SM/OubopW+/+G12RXfxk7N/MmELwLRMrnniGgLuQN5BBkF3cMiR6KZl0pJoyWsgai6zgrMoDZSO6ZiJ0J3uHhTcMRLRVJQHdzzIv3b8i1gmxlHlR3HJ/EtYUrpkzNZHbqe7pSy++9J37YjCFTcwt3DuoP1rw7UU+grJWll7kG+ibdwPOgOD6rDdvzhZ9D57cgVnYFnutngmTluibconWsvFwCDkCRH2hol4IxN+cPdmYFZK7b/HYe4599mcNtO0JlrH1GDQQjODmYlC08vO6E4e2/UYC4oWcGrNqeNyk6xsWsn3X/4+nz7205xcffKE6vPE7ie4+bWb+fzyz4844r+XiCdCbaR2xNZwPBOnMdaY18OkwFvA7MjssVR5UlBK0ZZsoyXekvcPP56J88jOR3hg+wN0pbpYWLyQC+deyHGVx41rbNPdm+/mzo138pEjP8K5c84dtH1gVoTeenelusY8MVrAZefmGkvI+FQSy8RoS7RN2OU6HB7DQ8QTIewNE/KEZsw4m2Q2yZ6ePXn/77TQzGBmstBMBpay+MzjdtLJr5/y9XGfJ2tlueaJawh7wnzz1G+OKnpFPjtIIh9xzOdB7jN80z7YLmNlaIo10ZXuyvuYtJnmid1PcP/W+2lJtOBz+Tiu8jhOqT6Fo8uPzmtunnWt6/jGC9/g5OqT+eQxnxz0mVYGK0cdQd6T7qEt2Tbq3DWl/lIqg5UzckxOMpukLdmW1/inkRCEgDtAxBsh7AlPeXqdiWApi+Z4c16W/0wSmoMnuFwzJC5x4TW8fRaCIQbnzjmXP274IzujO5lTMGdc532q4Sma481cefyVoz6ESv2lY5pgTEQoC5RR6C2kMdZINNN/XIuBweyC2dPe0vQYHmojtZRkSmiKN+UV1ut1eTmv/jzOmXMOb7S/wXN7nuPFfS/y3N7nCLqDnFB1AidXn8wRpUcMGdzRmerkJ6t+wqzQLD521McGffZl/rK80pSEvWHC3vCwD2uXuKgOV8/YCd4A/G4/NeEaKoIVtCXa6Ex15t0R39uRH/bYy1SE408FhtjRfiFPiL09ew9on89E0BbNAA4Wi6Y3i3CJv4SeTE+/SKWedA//+ch/cnrt6XzsqI+N+dxZK8tnHv8Mhd5CvnHqN0YUmnxa16MRTUdpjDX25eHq7XuYaSSyCdqT7URT0TH50ntnG31u73O83PgyiWyCAm8BJ1adyCnVp7C4ZDGGGFjK4lsvfouN7Rv5xqnfGNRIKPGVjHsul4yVoT3RTkeqA6/Ly+zw7Emf+XSqMS2TjlQH7cn2IaeFyKcj/81E1sqyt2fvsC5EbdFopoxcgeltpQ10BYS9YU6tOZVn9jzDew57z5hHkT+x+wlaE6185MiPDCsyglAdqp6UdB8F3gJC7lDftAgzUWTAzvhQE66hMlhJZ6pz2AfeQNyGuy+xY9pMs7p5Nc/tfY4ndz/JwzsfpsRfwsnVJ5MxM6xtXctVR101SGQKvYUTmjDMY3ioDFVSHixHkBnpKhsNl+Gy58Dxl9KV6qI91Y5HPH3W23ROTDcVuA03dQV1tCXaaIo3zegoPi00BwlucVPiL+knML34XL5BaULOqz+Px3c/zpMNT44p+WLWynLPlntYULSAZeXLhtzHwKA2UjvuVDBD4TJcY3K/TSduw903AVg0HaUj2TFqX0gvXpeXE6pO4ISqE0hmk6xsWslze5/jn9v/ialMTqs5jTNn989XF/FEqAnnP05nJKbbHTkZiAhF/qI3XU6z8VIasFMZjSVQ4ECjhWYS6TXHLSw7fFEpLCwsZaFw3g9Yn+j4Bre4KQ2UUuwrHtHP7HP5+vUhzC2cy+Lixfxrx7+4cO6FeT9gHt/9OK2JVj525OD+gd76zI7MPihcE5NBgbeAAm8BKTPVN6FWvv0IfrefFTUrWFGzgp50D2+0v8FR5Uf1+9xD7tC0zkKpmRn43X7mFs7NO1DgQKOFZhLpTR8yFixlkTbTpMwUGSvTt5420yN29PUKTIm/JC+RCLgDgzqrz68/nx+v+jGvtbzGMRXHjHqOjJnh7s13s7B4IUeVHzVou8fwMKdgzgGZiuDNhs/lY1ZoFhXBCtutlmgnZY2YpKIfYW94UAh5wBVgdmT6gyI0M4OBgQIzCS0004whBn63f8iQyt7BXWkz3Te6OG2miXgjeQtML37X4POfUHUCReuL+NeOf+UlNI/vfpz2ZDsfP/rjg1rQbnEzt2Dum64D+UBjiNHn4kxkE33pWcYiOmCHd9cVDD0Fs+bQJuKNMK9oHi6ZOd8NLTQzGJfhImAECLgDEz7XUELmNtycXXc2f9v8NxpjjSP2gaTNNPdsuYfFxfZc8QMJe8NaZMZIwG3/byuCFaTMlC066eiQc9Xn4jE8zCmcc1ClvtdMLjMt8EHb3IcIPpcPY4h/99lzzsYQg4d3Pjzi8Y/teoz2ZDvvXPzOIfsDJkMMD2V8Lh9lgTLmFc5jUdEiZgVnEXKHEAZbjvUF9TPuQaLRjIQWmkMEEcHnHtx3UuIv4fhZx/PE7idIZoeOWEmbae7dci+HlRzGEaVDzxunhWby8Lg8lAZKqS+sZ1HxImpCNUQ8EdziZk7BnBmTCkajyRctNIcQAdfQYnB+/fnEMjGe3fPskNsf2fkIHakOLl90+ZDWjIExZB+QZuK4DTdF/iLqCupYXLJ4RqdH0WiGQwvNIcRQFg3AYSWHURep46EdDzEwU0TaTHPf1vs4vORwlpYtHfa8OrxWo9EMhxaaQ4jhrA4R4fz689nVvYuNHRv7bXt458N0pjq5fPHlw5436NZjZjQazfBooTmECLgDgzqXe1lRs4KQJ8RDOx7qK0uZKe7beh9LS5eypHTJiOfVaDSa4dBCcwghIsMOpvS7/Zwx+wxe2vcS7Uk7MejDOx6mK9U1ojUDWmg0Gs3IaKE5xBipM/ncOediKYtHdz5KMpvkvq33cWTZkSNOFe0Wt46C0mg0IzLjRnyJyJ3AYudtEdCplFomIvXABqC3E+EFpdTHnWOOA/4PCAD/AD6tlFIiUgLcCdQDO4B3KaU6DsiNzFBGig6bFZrFsoplPLrrUdyGm2g6yjsXvXPE82lrRqPRjMaMs2iUUu9WSi1TSi0D/gr8LWfz1t5tvSLj8AvgY8BCZ7nAKb8WeFQptRB41Hl/SDOaMJxXfx6dqU7+vPHPHFV+FItLFo+4vxYajUYzGjNOaHoRO172XcCfRtmvCihQSr2g7Njc24B3OJsvAW511m/NKT9kGW0cxtHlR1MZrEShuHzRyH0zoIVGo9GMzoxzneVwGtCklNqcUzZXRFYBUeC/lVJPAzVAQ84+DU4ZQKVSap+z3ghUDnUhEbkKuAqgrq5u8u5gBmKIgc/wDZvE0RCDDy39ENuj21lYvHDU82mh0Wg0ozEtQiMijwBDZXC8Til1r7P+HvpbM/uAOqVUm9Mnc4+IDD2CcAicPpshp6BTSt0C3AL2VM75nvPNit/tJ5UePlvwMZXHcEzl6NmcfYZPZw/WaDSjMi1Co5Q6Z6TtIuIGLgWOyzkmBaSc9ZUishVYBOwBanMOr3XKAJpEpEoptc9xsTVP3l28efG7/XSluyZ8noBHWzMajWZ0ZmofzTnAG0qpPpeYiJSL2BMsiMg87E7/bY5rLCoiJzn9Oh8Eeq2i+4ArnfUrc8oPaSbL3aXdZhqNJh9mah/NFQwOAjgduEFEMoAFfFwp1e5su5r94c0POgvAd4A/i8hHgJ3YwQWHPJOVAFMLjUajyYcZKTRKqQ8NUfZX7HDnofZ/BRiUv14p1QacPdn1e7PjMlx4DA8ZKzPuc+iMzRqNJl9mqutMM8UMN2VAvvjdfp2xWaPR5IUWmkOUic5rot1mGo0mX7TQHKJoodFoNAcKLTSHKBMVGj0HjUajyRctNIcoHsODW8YXC+IWNx6XZ5JrpNFoDla00BzCjNf9pd1mGo1mLGihOYQZr/tMC41GoxkLWmgOYbTQaDSaA4EWmkOY8Y6l0UKj0WjGghaaQxiPa+wBATpjs0ajGStaaA5xxuo+0xmbNRrNWBlVaERkhYg8LCKbRGSbiGwXkW0HonKaqWes+cq020yj0YyVfPwmvwE+C6wEzKmtjuZAM2aLRguNRqMZI/kITZdS6sHRd9O8GRmLcOiMzRqNZjzkIzSPi8j3gb/hzHAJoJR6dcpqpTlgeF1eXOLCVKMbqzpjs0ajGQ/5CM2JzuvynDIFnDX51dFMBz6Xj3g2Pup+2m2m0WjGw4hC40ydfJ9S6n8PUH0000DAHdBCo9FopowRo86UUibwngNUF800kW+/i87YrNFoxkM+rrNnReSnwJ1ArLdQ99EcPOQTeaYzNms0mvGSj9Asc15vyCnTfTQHET6XDwMDC2vYfbQ1o9FoxsuoAzaVUmcOsUxYZETkchFZJyKWiCwfsO3LIrJFRDaKyPk55Rc4ZVtE5Nqc8rki8qJTfqeIeJ1yn/N+i7O9fqL1PhgREXxu34j76IwAGo1mvIxq0YjIV4cqV0rdMFT5GFgLXAr8csD1lgBXAEuBauAREVnkbP4ZcC7QALwsIvcppdYD3wX+Vyl1h4jcDHwE+IXz2qGUWiAiVzj7vXuC9T4oCbgCJLKJ4bfrQACNRjNO8sl1FstZTOBCoH6iF1ZKbVBKbRxi0yXAHUqplFJqO7AFOMFZtiiltiml0sAdwCViD+w4C7jLOf5W4B0557rVWb8LOFv0QJAhGa2fRg/U1Gg042VUi0Yp9T+570XkJuChKasR1AAv5LxvcMoAdg8oPxEoBTqVUtkh9q/pPUYplRWRLmf/1twLishVwFUAdXV1k3YjbyZGEhq/y68zNms0mnEznuzNQaA2nx1F5BERWTvEcsk4rjtlKKVuUUotV0otLy8vn+7qTAt+lx9haGNPu800Gs1EyKePZg12lBmACygHbszn5Eqpc8ZRpz3A7Jz3tU4Zw5S3AUUi4nasmtz9e8/VICJuoNDZXzMAEcHv8pMwB/fTaKHRaDQTIZ/w5oty1rNAU46baiq4D7hdRH6AHQywEHgJEGChiMzFFpArgPcqpZSIPA68E7vf5krg3pxzXQk872x/TCml0AyJz+3TQqPRaCadfFxn31BK7XSWPU5fx+8nemER+TcRaQBOBh4QkYcAlFLrgD8D64F/Av+llDIdcfsEdv/QBuDPzr4AXwKuEZEt2H0wv3HKfwOUOuXXAH0h0ZrBDNXhb2Dgc40c+qzRaDQjIaM18EXkVaXUsTnv3cDrSqklU1256WD58uXqlVdeme5qTAvxTJzt0e39yoLuIHML505TjTQazZsFEVmplFo+1LZhLRpn0GQ3cJSIREWk23nfxH7XlOYgYqjIM+0202g0E2VYoVFKfVspFQG+r5QqUEpFnKVUKfXlA1hHzQHCEAOf0d9NpoVGo9FMlHz6aK4TkfeLyFcARGS2iJwwxfXSTBMDrRqd40yj0UyUfITmZ9gd9u913vc4ZZqDkFyh0RmbNRrNZJDXDJtKqWNFZBWAUqqjN2ml5uAj11WmrRmNRjMZ5GPRZJyZNhWAiJTDCPnkNW9qckOcdcZmjUYzGeQjND8G7gYqROSbwDPAt6a0Vpppw2W48Bq2waoDATQazWQwoutMRAxgO/BF4Gzs0fnvUEptOAB100wTfrefdDqthUaj0UwKIwqNUsoSkZ8ppY4B3jhAddJMMwF3gLSZxpDx5FzVaDSa/uTzJHlURC7T87gcOvhdfm3NaDSaSSOfqLP/wM4TlhWRJLb7TCmlCqa0Zpppw+/2k7Ey010NjUZzkJDPxGeRA1ERzczBbbgJe8PTXQ2NRnOQoJ3wmiHxGHqgpkajmRy00Gg0Go1mStFCo9FoNJopJS+hEZFTReTfnfVyZ5ZLjUaj0WhGZVShEZGvYc9g2Ts1gAf4w1RWSqPRaDQHD/lYNP8GXAzEAJRSewEdiabRaDSavMhHaNLKnu+5N6lmaGqrpNFoNJqDiXyE5s8i8kugSEQ+BjwC/GoiFxWRy0VknYhYIrI8p/xcEVkpImuc17Nytj0hIhtFZLWzVDjlPhG5U0S2iMiLIlKfc8yXnfKNInL+ROqs0Wg0mvGRz4DNm0TkXCAKLAa+qpR6eILXXQtcCvxyQHkr8Hal1F4ROQJ4CKjJ2f4+pdQrA475CNChlFogIlcA3wXeLSJLgCuApUA18IiILFJKmROsu0aj0WjGQD4paHCEZaLiknu+DQAD06cppVblvF0HBETEp5RKjXC6S4CvO+t3AT918rJdAtzhHLtdRLYAJwDPT8pNaDQajSYv8ok66xaR6IBlt4jcLSLzprBulwGvDhCZ3zlus6/kJPmsAXYDKKWyQBdQmlvu0EB/60ij0Wg0B4B8LJofYj+kb8dOqHkFMB94FfgtcMZQB4nII8CsITZdp5S6d6QLishSbBfYeTnF71NK7RGRCPBX4APAbXnUf1RE5CrgKoC6urrJOKVGo9FoHPIRmouVUkfnvL9FRFYrpb4kIv9vuIOUUueMp0IiUos9o+cHlVJbc863x3ntFpHbsd1gtwF7gNlAg4i4gUKgLae8l1qnbKi63gLcArB8+XI1nnprNBqNZmjyiTqLi8i7RMRwlncBSWfbpD6URaQIeAC4Vin1bE65W0TKnHUPcBF2QAHAfcCVzvo7gceccOz7gCucqLS5wELgpcmsr0aj0WhGJx+heR+2m6oZaHLW3y8iAeAT47moiPybiDQAJwMPiMhDzqZPAAuArw4IY/YBD4nI68BqbMukN8T6N0Cp09l/DXAtgFJqHfBnYD3wT+C/dMSZRqPRHHjEbvxrelm+fLl65ZWBEdQajUajGQkRWamUWj7UtlH7aETEjz1WZSng7y1XSn140mqo0Wg0moOWfFxnv8eOHjsfeBK7U717Kiul0Wg0moOHfIRmgVLqK0BMKXUr8DbgxKmtlkaj0WgOFvIRmozz2umkhSkEKqauShqNRqM5mMhnHM0tIlIM/Dd2yHAY+MqU1kqj0Wg0Bw0jCo2IGEBUKdUBPAVMZcoZjUaj0RyEjOg6U0pZwBcPUF00Go1GcxCSTx/NIyLyeRGZLSIlvcuU10yj0Wg0BwX59NG823n9r5wyhXajaTQajSYP8pn4bO6BqIhGo9FoDk7ymY8mKCL/LSK3OO8XishFU181jUaj0RwM5NNH8zsgDZzivN8DfGPKaqTRaDSag4p8hGa+Uup7OAM3lVJx7AnQNBqNRqMZlXyEJu1MCaAARGQ+kBr5EI1Go9FobPKJOvs69nwus0Xkj8AK4ENTWCeNRqPRHETkE3X2LxFZCZyE7TL7tFKqdcprptFoNJqDgnzmo7kfuB24TykVm/oqaTQajeZgIp8+mpuA04D1InKXiLzTmQxNo9FoNJpRycd19iTwpIi4gLOAjwG/BQqmuG4ajUajOQjIJxgAJ+rs7djpaI4Fbp3KSmk0Go3m4CGfzAB/BjZgWzM/xR5X88mJXFRELheRdSJiicjynPJ6EUmIyGpnuTln23EiskZEtojIj0VEnPISEXlYRDY7r8VOuTj7bRGR10Xk2InUWaPRaDTjI58+mt9gi8vHlVKPA6eIyM8meN21wKXYc9wMZKtSapmzfDyn/BfYbruFznKBU34t8KhSaiHwqPMe4MKcfa9yjtdoNBrNAWZUoVFKPQQcJSLfE5EdwI3AGxO5qFJqg1JqY777i0gVUKCUekEppYDbgHc4my9hvyvv1gHltymbF4Ai5zwajUajOYAM20cjIouA9zhLK3AnIEqpM6e4TnNFZBUQBf5bKfU0UAM05OzT4JQBVCql9jnrjUCls14D7B7imH0MQESuwrZ6qKurm6Tb0Gg0Gg2MHAzwBvA0cJFSaguAiHw23xOLyCPArCE2XaeUuneYw/YBdUqpNhE5DrhHRJbme02llBIRle/+OcfdAtwCsHz58jEfr9FoNJrhGUloLgWuAB4XkX8CdzCGZJpKqXPGWhmlVAonj5pSaqWIbAUWYWeMrs3ZtdYpA2gSkSql1D7HNdbslO8BZg9zjEaj0WgOEMP20Sil7lFKXQEcBjwOfAaoEJFfiMh5U1EZESl3xusgIvOwO/K3Oa6xqIic5ESbfRDotYruA6501q8cUP5BJ/rsJKArx8Wm0Wg0mgNEPsEAMaXU7Uqpt2NbBauAL03koiLybyLSAJwMPCAiDzmbTgdeF5HVwF3Ax5VS7c62q4FfA1uArcCDTvl3gHNFZDNwjvMe4B/ANmf/XznHazQajeYAI3YQl6aX5cuXq1deeWW6q6HRaDRvKkRkpVJq+VDb8hlHo9FoNBrNuNFCo9FoNJopRQuNRqPRaKYULTQajUajmVK00Gg0Go1mStFCo9FoNJopRQuNRqPRaKYULTQajUajmVK00Gg0Go1mStFCo9FoNJopRQuNRqPRaKYULTQajUajmVK00Gg0Go1mStFCo9FoNJopRQuNRqPRaKYULTQajUajmVLc010BjUajeTORyWRoaGggmUxOd1WmBb/fT21tLR6PJ+9jtNBoNBrNGGhoaCASiVBfX4+ITHd1DihKKdra2mhoaGDu3Ll5H6ddZxqNRjMGkskkpaWlh5zIAIgIpaWlY7bmtNBoNBrNGDkURaaX8dz7tAiNiFwuIutExBKR5Tnl7xOR1TmLJSLLnG1PiMjGnG0VTrlPRO4UkS0i8qKI1Oec78tO+UYROf9A36dGo9Fops+iWQtcCjyVW6iU+qNSaplSahnwAWC7Ump1zi7v692ulGp2yj4CdCilFgD/C3wXQESWAFcAS4ELgJ+LiGsK7+mAoJSa7ipoNP2IpbLTXYVDDpfLxbJlyzjiiCO4/PLLicfjI5Y3NDRwySWXsHDhQubPn8+nP/1p0un0AavvtAiNUmqDUmrjKLu9B7gjj9NdAtzqrN8FnC22bXcJcIdSKqWU2g5sAU4Yb51nAlnTYmtLD1uau+mKZ6a7OppDnGTGZFtLD9taYmxr6SFrWtNdpUOGQCDA6tWrWbt2LV6vl5tvvnnYcqUUl156Ke94xzvYvHkzmzZtoqenh+uuu+6A1XcmR529G1sscvmdiJjAX4FvKLt5XwPsBlBKZUWkCyh1yl/IObbBKRuEiFwFXAVQV1c3mfcwaWRNi+2tMZIZ+8e8qz2ON2pQHvFRHPS8OX3GmQS4fGDorsI3E1nToqk7RUcsTa+BHUuZbG7uoa4kSMg3kx8rk8v1969j/d7opJ5zSXUBX3v70rz3P+2003j99deHLX/sscfw+/38+7//O2BbPf/7v//L3Llzuf766wkGg5NW9+GYsl+4iDwiImuHWAaKx1DHngjElVJrc4rfp5Q6EjjNWT4wWXVVSt2ilFqulFpeXl4+7vNkpqhFlxkgMr2ksxZ7OhK80dhNc3cS03qTuNXMLHTsgJY3oGkNtG+DeLtdrpmxKKVo7Umxsamb9p79ItNL1lRsb43R0p2angoCWBakuqG7Cdq3Q0/LQf29ymazPPjggxx55JHDlq9bt47jjjuu3/aCggLq6urYsmXLAannlDU9lFLnTODwK4A/DTjfHue1W0Rux3aD3QbsAWYDDSLiBgqBtpzyXmqdsimjuTuFITCrwD/YwrBMMDNgZZxX0173BCFQNOw5e0UmlRlexLKmoqkrRUt3itKQj9KwF49rhloJiQ7oagDL+fErC5Jd9oKANwT+Qntx+/ofmjZp7UlR4PcQ8bsxjGGsODMDmThkkpBNgLjsz9kbBLcf3ozW3zTTncywrys54vcQQClo7EqSSJvUFAdwjfQ/EtfErdlMAtJxyMTs12yi//ZkJ0T3gC8CwRLwFQ57zaxp0ZPKEvK58/79jMXymEwSiQTLli0DbMvlIx/5yLDlvW616WTG2bgiYgDvwrZaesvcQJFSqlVEPMBFwCPO5vuAK4HngXcCjymllIjcB9wuIj8AqoGFwEtTWvd0D50d7aRbFVURN16x9gsLI1gbyRIorAWjf6xCOmuLTDqbn6VkWdDSnaK1J0VR0EN5xIfPZUA6Br7wBO5sEjAz0LXbEZThUJDusZfoHnAHwF8A/kJ6lI+dbTEsCzrjGUQg4nNR6DGJuE1c2QRkk7bAWCO0YMWwz+sN2uLjCYLHP65bSmVNooks3ckM8bRpn15AkP3rznNWkJx1u7yqMDDj3UyprMm+ziTdybFZBV2JDMmsSV1JEL8n53udSUL33v3fAzHA8IDLA4bbXlweu8xw7V93eez/azpm/4/TcftVmXnURkEqai/isht2gZK+30TGtGjtSdGWY6UFvC4KAx4KAm587umNIVJKYSqF4XyHRKSvL2YgQ5UvWbKEu+66q19ZNBpl165dLFiwoO8aSjF8422CTMu3XET+DfgJUA48ICKrlVK94cenA7uVUttyDvEBDzki48IWmV85234D/F5EtgDt2NYQSql1IvJnYD2QBf5Lqby+lePGSHbiTjSTBhp6hMpCHyFvHh9xot3+ARXV9X35xyoyuSgFHbEMXR0dlGSbKfRZuMsW4AkWTE9fTqzNFo6xfvzZBPQk6G7dQ2OPictbgGF4ETOJkU2SNpO0oGgFgl43YZ+bkM/NiI1RZdmt30xsf1mvxePpFaAQuL1D30oqS3cySzSZGbJ1bz+ohmpUDC7b0RZjXlmYgHfmBUOalqK5O9nv4TtWUhmLLc091BQFKPYb0L0P4m30+yyUBWbKXg4EyrTrEG8jjZtWM0SHFcJy9W9sJNImibRJYxf4PIYtOn7PAf9fZU2LrKWwnH+C3UiRvm2G7Bef4Tj77LO59tprue222/jgBz9IJpvlmmuu4YMfvBKX10cyY2IphSGC35ia+xMdLtuf5cuXq1deeWVcxzbu2kx3e2O/spKQl5KgN39vTbiSVKCC7W1xMtlx/m+sDJ6evbjSOdaDuEgVLcDw+PG4BLfLwOMSPC4Dt7H/vduwXydFkLJp24pJjb+ztDOeoaUn/4eQYItOyOcm5HPhHm8LzWu7WixfEd1pk2giQ3cyO+n9YC5DmFce6t/qn0aUUnTEMzR2TVKfn7JwJ1opoYOKkGfMXkulIJk1+/onAx4Xfs/E3G3JjEVHPE1PTli2cgdJeYpoTPuZVTy89e9xC9m2Bg4//HCMUR7wE8G0FBnTwlKKeMokmsxQFvbhdtnXqygporm9E9gvPoZASVEhXdHuvrpZjkjt2rWbT37iv9i4cSOWZXH+BRfwre9+D59vv3vaEMn7e7hhwwYOP/zwfmUislIptXyo/We23X4Q0B5Lk0ibVBb68eTx0Et17qOhYS/Z4Gy7P2EsKIU70Yo73gQMaG0rE290B6nCBZiWC0bxtbsM2S9E4xGknhbbRaLGHyDRFkvTHhtbrL8CYukssXQWX8N2yrb+DbxBkosuRhXP7XNtGc4PEwFDcNwS+91biXg7seYWElnIeovI+ktQw/w/TEvRlchQEhraChoJ01J9lo3XnfMAtSzbFdi7ZBJgpu1+K2/Y7svyBMfV36SUIm1apLIWaWfpXc+Y1pgtmL2dCSoiPtwDTElXshN3vBGx0kSBdDrDrMIAHtfwdc5ayhaWtEUiY5LKmINsQUMg4HET8Nqi43e78voYEmmTjniGWHqwGzCbivG9J1p5uiHLh48p4NLjZqN8hbZrL4dMVmFZilTWRBBchu1ucsnkNM4sR2BMx5XV2pOiK2EPZUhmLWqK7M+vV2TA/s4rpbAUNLd3OnXbvw1gVk0Nf7n7nhGvnTUVltu2bCYbLTQHgETGpKE9TmWBn+AIpncqa7G3M4FlKXzpzWSDs8gG84uCM9I9eGJ7EXP4HERipvB27yZdWD/q+UxLYVpqUKTbQHIFqSjoochj2VZMumf4g5SCtX+FNX+BmuPg6Ctst2HO5ubuJNEx9gv04unZQ+kbtxNueArLHUSsNMEtDxAvX0bn/IuJVS4f9AAZ8R6TrbiSrSh3kKy/BNNX1Hd8MmPyzX9sYPXuTg6vKuCsxRWcurCMcL59L0qRTSbZtbeL+gIDt5WCrLMM5YLLJvv3b3iCtuh4Q7YADXB9xFJZkhlzv6iY9uuwYmJl7fPm+fnc/9pebnl6G4dXFfCl8xdTGvZhZGJ4evYiZv+O+WTWYld7jMpCP2HHpZzKWiQzFslMlqRTx9Gw1P7GBOwXHr/XsC2eAcLTk87SEcuQzAztuk2bihufTfDC3iyHlRj8dlWUrvhmrjomiPIVYvqKsbyDrRyFImvZFRL2C47LGLvoWEqRNS1MS6Gw3Y6N0SQZ06Io6CXsc7OvK8GejoQtNu6Rzz9WWzSZsdjXlaAk6KWqKDDGo0dHu84GMNmus1wEx5U2RMs35YQqmwP+H5Y7RCYyG+UaprVsZfDE9uFKdeZdz2ygnGyoKu/980Ip3IkWQukWioN2VNiQJDrgie/C7hegfLEdgmpmoH4FHP0erPKlNHUn+7k18sWdaKVk4x0U7PwXyvDQOf8SOhZcCsqkcMdDFG5/AE+yjXSoiq55FxGtOxfLM44xBOLC9BbS4yrihn9tZ01DF+cvncXavV00dCTwuIQT55Zy9mEVHFNXjEsUYqYRK42YGcTKIGYKw0wh5n5B8bsNqouCI/cxjfohBMAbIusOsicuRNMusEz7mlYWVBaxBi77t/XWRRlelMuH5fajXD573eWzO+sd7l29h18/s50lVQVsa+3B7zb48qlFHFs6uhUa8LhIZ61B3/eh6EpZ/Hp1ChH4z2P9BEZ5yAoQ8LrxewziKVvAhiOVVVz/bJyX95l8armft8338ItVKe7ZlObsOR4+f6Lfdr+Kh6y/CNNXhNndxsLFi0e8vuEIjmHIiBaCUoqsZYtMb/deezxNRyyNyzCoLPAR9LowDCGRNtnTmUDAEZvJiSztTmZpjqbwuIT6svzcuGN1nWmhGcCUCY1SuJJteHv2EkruozDViBFtgFA5yeOuYm9Mhv/RiYtMqBrTXzzofJ5409g72YFMeHb/8+UQd1qKwXwCGQCsDN7oLozs/g52n9ugOOQlktuyb3gZHv+Wbe2cdDUseYcdfrrubntJdZMqW0rbgsvGZHUYqS5KNt9F4ba/I0rRNfdC2he9a/D9WVnCe5+jaNt9BNrfwHIHiNadQ+e8i8iEhxzLOyyJrOIrT8V5vdnkmtOrOHNJDZgptjR18+jmdp7c1kM0ZVHsF86a4+Hceg/zi0f/Afs9LmoKAxOK+o2nTZqiSbKWwn7sTeJvXNxYLh93vZHkV690sGJuIV84Zz7NLc1847G9NHRbXHmEjyuWeCfFBfPkrgw/XZmkO2239OsKDL52aoDayDCfpVJ4ozuJ7HmaYNMrpCO19NScTqzyWDt6LYdEVvHVp+K81mxyzQl+LpjndU6h+NP6NL9bk+L4KjdfWRHoJ24JibBw8WEgLlQe99grOi6RvqgupZTTD6NQzv8nYyqaokmSGZOI30N52IfLEHxuAxHb6khlTfZ2JlDYYuOdoNi096Rpj6cJeFxUF+UfBamFZoJMVGhijVvw9OzB27PHfo3tdd7vxchxa1kuH0Sqka6dZMI17D3hOjKR2hHPn3FHuG+Xh0w6zfxgjLkRk1L/eH3DQqpwPqY7wJ7OBBv3dfNGUzcbG6PsbIvj8xh88KR63nZU1YgPDCMTwxvdBWrolDh+t0GRX4i8fiu8fgcU18PZX4WSef3vLRWnZ/U9hDfdjSfRQipSR8eCS+me/ZZBD4j9145TtPUeirbcjZFN0V13Fm2L30M2VImlFCnTdotEvINblb6OTRRtvZ/InqdBmcQrl9M5/+3Ey48Ztd8jkVFc91Scda0mXzwxwNn1g+uXMRUv78vyrx0ZXtybJWvBvCKDc+o9nDXHQ2lg+AdEyOumqtA/ro7z1liKzilOT3THhhS/eS3F6bPdfPnkQF/ARSKj+MHLCZ7YleWkajdfPClAxJtzE5ZJqPFFirY/gL9jE4mSw4iXH0O84ljSBXP6fe4dSYufvJLk6YYsC4sNPn9igI6k4lvPJcgqxZdODHBK7f7P3dPdQGTP04T3PIWvezcKg2TJYXi7d+PKdGN6QvRUr6C75nQSZUcSNw3+2/kffuFEP+fU2yLjdxt9FtA/tqb50StJFpe4+MbpQQp8zn1KmEUL7bBgxECJC7Bj2RUGjPB/s/sGBUvtjyQDiCaytDpBLxURH2G/G0MEr9vo++5mTdv1mckq9nQmUKhxi41S0BS1PQcFfnsohMuYumAALTQDGLfQvPEA1j1XYyQ7+4qUGGSClWTCNaTDNc5rNZlQDdlAKSIGwdbXqHjpu4iVpem4a4hVnTTk6WNpxTeft038XCJeob7QYG6RQX2hi7mF9mvYO/S3PZZWbGgzncViQ7tFT8o+Z8jnYnFlhMNmFfBGYzev7upgSVUBnzprITXFg/22rkQrntg+Rmoxe3r2MOuVm/B3biaz+CI8Kz4xKMghlbXY25UgayqwskT2PE3x5r/ii+4g4y+lc/4ldM45nycbvbyyL4uZTXFaz794R/JuClQ3TxoncIvxLjaZNSRNSJmKdM7HNL/IxRdO9A9pUbiS7RRuf5DCHQ/iTnWSisymq/6ttjWkTERZtutJmaAsMlmTf2xO0BLPcnadiwWFIMrEcgeJVR43ZGMhmrJ4YleWh7eneaPdwhA4bpab9y7xckT50C3IiN/NrIL8g0FSWYvmaHJEN9FkcPu6FL9bk+KMOjfXnjR4QKZSins3Z/jl6iRlAeGrK4IcFoxSsPMhCnf8E0+ilUygnHjFMfjb38DXvQuArK+YeMUyYuXH8K/Ukdy0xk8yq/jgET4uP8zbd52mmMUNz8bZ1G5x9cIOPhR+kcI9T+OLbkchJEqX0lNzGj3Vp9j/QytDsHk1kT1PEdr3Aq5sgoy3iAetE/lD/ETOPfFozphjR16FfW4qIn52tcccaxCe3p3h288nqA4bfPuMIOVBo7/QDIkdZaJEHKu8d0CVDLKATEvRHE0RS2cJel1URPy4XbYF5HUZgxqRvaHIGdMRG0tRXRTAN4ZIPNNU7O1KksqalIV9FAVtwXaJ4NNCc2AYr9A0bnyJDffeRH1NNb6SWltYQpXDtsZzccebqXrp2/g7N9O26N20H/5ee2yHw55ui688HWdvt8Unj/Nz6mw3OzottndZ7Ogy+15zG7LlAaG+yBae8qDB1k6TN9pMdnbZvmAB5hQaHF7uY+Hsag6rKqSmONDXelJK8dgbzfzqmW1ksor3nVjHJctq7B+8svD0NIzcL6QUkd2PUfHaL1CGm6ZjPkWs+hT8bhclYU/f+KJExh4QOMhtqBTB5lcp3vxXgq2v00OQ27Ln0OEq42NyDxW0s9J1FHf4r6DBvwCfC/wuwecGn0vwuwWvx0vWX8w9q/fRnczwvqU+3rPEO2TIs5gZwnueomjrffi7to76PxuOdLiG2KwT6Jl1IsmSwwd1zu+KmjyyI8O/tmfoSSu+c0ZwWLEpDHioiPiG3JZLVyJLS3dyMh1kQ/L7tSluW5vi7DkevnCif/hR/8CG1gz3Pvs6l2Qf4m3ul3CprB2MMfdtxGad0Pe5uBOtBJtXEWxZjb9pFZ6MHQq/1ZiDq+ZY3LOPI1m6BI/Pj1KgupsINDxNcvOT1Gfs/1NP0WIStafTXbMCM1A2bJ3ETCENr9Dw+uOcmF2JXzJkAmX01JxGfPZbqJx3FG6XQU86y77O/d6H1U1ZvvZ0nJBX+M4ZQcoLC0cRmtGwRaEno2jqsYWjLOiiKGA4kWziROYN/nxNpZyACSFrKRq6MpgKagrc+AdYNsrwDDpFKmOxryuJpRSVBX5Cvv3fT5/bNeL/NBctNBNkvELzyPomPnn7SkQUVx/j57y5Y0t0KWaa8td+TuGuR4hVHEfj8s9jeSOsbspyw7N29M7XVgQ4unLoh5JSipa4Ynuv8HRabO8y2R21yFi25XN4qYvDS10sKXOxuMRFyLF6TG8hmYI5Q563PZbm5ie38vy2NhZWhPn0GXNY5GkZMbrNyMSoeO3nRBqeJF56BE3LP092wAPA73ER9rlp60kN+4Dc2G7ym9eSmM2b+JTv75zFyxhYJIoX07bkShLlRw1bB8tbQDpSB2IQTWS45eltPLmpZUTrxvkg7eg9K2O7RMSFEoN41uBbL6TZ2AmfPiHEybU+221iuCiPBClSnZg7niOz9Rl8za8hKovpiRCrXE6s6kTiFcf2CzzoSFp87tE4bUmL758ZYlHJ0PUpCXkpHSZsOmspWrpT/QInJJvE270TX9d2fF3bcaW7SZQdQaziWLKhWcN+XiOhlOK2tSn+sC7NufUePnfC8CIj2SSRhicp3P4A/q5txAjw5+zpbJ11Ie8+aT7+ITrylVI8tD3DL1fFWWDt5NPV6zlBrSHQvh7DymIZXqg6GjIJjGY7/WGyaAEv+0/hv3cdR8JfwVdPDbJ4mM+wl86kxZeeiLM7anHjyXAGrxLe8xShplcRlYWCaph3Fsw7nUbPbLpT+83iLR0mX34ijqXgd/9WxdLF4xcaS0FL3KIrpfC5hKqwQW8wqtswRh37lTYtAuFCjli6lHTG/p6+/Z1XcO01nyDkdfHUU09z+bvfQ/3cuSTiCSoqK/jsNZ/n9HPOpyma4hc/+A53/+k2ysrLyGZNrr/hRi655BK8boM//OEPfO9738M0TdxuN8cffzw33XQTRUVF/eqghWaCTKSP5pU167j+X7tZ02JyUrWbzx7vp2QEX/wglKJgxz+peP2XZAJl3Fn1Bb6+voqaiMGNpwWpjozdF5u1FJ0Ji9LgYDO8337BSrLBymGqpXhmSyu/fHILsbTJe5f4uOJw75DjIfztbzDrle/jTrTQdth76Vh0eT/rrD8Gg8b7AA1Rk9+uSfH07iyFPuG9S7xctMBLKNmIK9lhWwoj3cswUXXPb2vj549vGdW6GUgsrbj2yTib202uOyXAabP3W6l+t8HssgI7Gitri297VyepbS8SanyRUNMruNJRlLiJlx1JrOoEYrNOJBusoDlmcc2jMRJZ+J+zg9QXDv05lYXtDN396pTK0NbciKtzqyMq2/B1bbfDip3P1HQHMF0BvKl2ANKhKuIVx9pL2ZGoPCLulFL835oUt69Pc/5cD589fmiR8fTsoXD7PyjY9QiuTIxUQT2dc99GZ81b+MNGgz+sSzO3yOArK/p35DfHLH7wcoKVjSZHlru45gR/33bJJgm1raM8+jruvSvBcJGoPY2m8lPIhKsBuzFywzNxOpKKTx7n58L5Q4tye8IWmb09FtefGmR51f4GW4GRoLL1Zdj6GOx9FZSFClcSnXUy0VknkSw9HMTFnm6La5+I8c1zZnHMkgUEPfk3JLMWxDOKRFYRy9i/yxK/QWlg/9gtj8vAlUfj1EJRUFJJa/M+APY1tfDeD36YZctP5Jtfv45Xnn+GH/7ox9x1z/0gwmuvreZdl13GV7/3Y95y5ln87iffIxKO8JlrruGNDRs47+wzaWpq4uGHH+a6667jvvvuo6amBtM0ufXWW1mxYgWLB0TZaaGZIBMNBuhs3cfdm9L89vUUfrfwqeV+zqgb3X2Wi6d1A0XPfxtftodbwh/nLW85t8/6yBd3vIXw3meI7Hkab3QXLUddRXTOeSMek47UQaCI8ogP01K0du8PU3XHGol1NvLzV5M8tjPLvCKDz50Q2N8SVybFm/5K6Rt/IOsvo/H4L9iCMAymr5hMuAZXsr0vcq41YfGHtSke3JbB54J3LvZy2WE+Qnn/oMWOzguUDrvHmKwboCetuPaJGFs7Lf77lAArcjqglTtIbe1sgoXOWKfoXojZ8/ElMiZN0RSZTAZ/+wZCjS8RbnwJb08DAKmCelIF9cRMF8/ugwxuTq0LEPJ7UOJGGR6U4QbDhTI8hAMBgl4D1bmbTPMmjPatuHMyP2QCFaQK55IqnEeqcC4t/nq+tirC6haLs4qaeE/hOpZbr1HYvgbDTKHERaLkcEd4jiFVNH9QlJ9Sit++nuKODWkunOfhM8d58KY6cCda8MSbcSdacCda8EZ3EWxbixIXPdWn0Dn3bSRLl/ZrDLy8L8u3n7fD9z9/QoAVtW4e2JrhV6ttl99Hj/bz9gWefkEbAtQUBwgM6DdojPbPvdaVsvjWcwlebTK5cJ6HTxznx5vTCGqNW3zh8TitcYsbTw+yLMcr4HYJdcWh/SHliU7Y+RzseArVsBKxMmR9RcSqTqKn6mQawkeQcJVQWjuPWWGD4qe/jtG8btD3RinbcjGd194kC+IMEHYbsL+KThqZ3reVS+HcGwFlJ99VpvNq9S3h6oV07lwP2O+3bdvBSedfxq41z/HiSy/yg1/8H/f98RaShfNp6U5x6//9jucee4i/3X033/rGDYRCYT5zzTUAzKur5fXXX+eyyy7jhhtu4Mwzzxx0PwPRQjNBJiu8eVfU5PsvJHij3eIts918crmfQt/oFkl3WvGNZ+PsamrnzsKfMD+1no55F9N6xIf7jWEYCleynfAeW1wC7RsASBbOR7m8BNo30LL0w3QuvHTIYwUoDPkonr0Et98enLajNUZ33B7kaWT2p5F5bk+GH72cpDOleNdhXj60MEndqu8TbHmN7prTaD76v4Yc4NZ7pUyoqp8vvSee4O4XN3HvG92YCi6a7+W9S712fqy8MUgX1GF5C/pfTRhycGI+1k13WvGlx2Ns77L46ooAJ9d4AMH0FZH1l1JUVEht8QCrINkFnbvAymJa0NLT/6Ho6W4g1PgSocaXcCdaEZXFymZIpbN4JItPshgjhKsrw00qModU4VzSvcJSMLff572xzeT6Z+J0pRVvX+Dl9eYsmzssBDiu3OI9ZVtZIWsoalvV1x9leguIly8jVnEspq8Qd7yFtTv30d3exBGBdua42nAn2+zgiBxMTwQzVEFP1Ul0zjkf018ybN2bYxY3PhvnjXaLugKDXVGLYytdfPb4ALPC/f/XAlQXBYYc4GxZsLsz3m9wp2nZ7r3b16dZVGLw1RVBKkMGzTGLLzweozOp+OZbBveHVRf5h89HmI7RuekZXDufIdT0CkY2gekO8cZ5f6Jk9kLazABzX7mRQNs6lFOv4YTFJQoDex1U35dSyHlVyt5WugBO+eSIWTXCC1fQvflZwLCj3cSgZPEprHrqAdZv2s7Pbvkdd9/5e3ZlCu1+0G0b+PRVH+bV19fwzRv3C82rL7/Muy6/jD179lBaWsr27dspLCwc9rq96BQ0M4S6Ahc/PCfEnRvS/H5ditdbYnzmeD+n1NgtYmX4sDwBjGyyr7+jIWrylacTNMYsPn1CJVb9t+hY9zuKt96Lr2sbjcd/adDYEFeqk/De5wg3PEWgbR2CIlVQT+vhH6Cn5jTbxWBlmLXyB5Sv+y2udJS2JVf2a21G/G5KQz7bFda1EzyLwOVhdkTY3byVTKZ/f8wpNR6OKnfzy9VJVr6xjU9tvwmfdNB0zKeI1p07vFtLPLYYeEKAnRn476/v4y8rdxNPmZyxoJgPLRGqg2McrCkeUoX1KPf+yDiv26A45KE46GVfZ7IvjUcvJ88rZWlVAb98ahu3rW3huT1ZvnCin3lF9oMtmrItmR1dFl8/NcAJtSGy/hKy/hIw3BgGQ0eF+Quh/DDo2Ikr3c2sAj9BT5Zmp7M+E6mlM1I7SPA3tpt88bEYpUGD/znTT7FPDRpUibLsvq4RGhz/3Jbmx68kKfELPzw7xELH4myImjy+K8tjOzN8bsNC3MZCjq96F289MsYZrrUUtq0i2LyKyJ79s6uXKBdd3lKCkQoSwSPIBirIBMvJBsrJBCvIBsrw+kPUFgUJK4ueriTmCFFvFSGD/zk7xC2rkzy+M8tnj/dz4byh+zJHyqLR+9nvbo/39e+5DOHfj/KzuNTFd19IcPVDMa5a5uP3a1P0ZBTfOTPI4aX9P7fCgGfkpLfeEOEl57F71qk0ZVIEW1YT2vscYqWpUk1UGkL38f9Bu/hAmRhYuLHwiIVPLAwsDGX1uTKHRQxnygTX4Ne+dWP/fk7mBrNkYV90XO95VKSaNqOLmPKyNVmAqSxmFfjpGTCA+qc/+RF3/ul2Cgoi3HnnnYP+B2vWrOEDH/gA3d3dfOtb3+Ld7373yPcwCtqiGcBUDNjc0mXw/RdibOvIcPaiYj562gLCASeayDLxdu9k9e5Obnw2jssQvrYiwJEV+78Ykd2PU7H6p1ieEPtO+DLpcA3hvc/ZlkvLGgSLVGQ2PTWn0V1zGpnI7EF1QJmUv3YzRTsepGvO+TQvu5qgz0dZ2ItvYBy+N2ynUY82kM5maWgfnLEAINT4EuUvfY8uy8/HUtfQFFrULxV+7isiKCPnoSJCZzxNdzLL8jnFfPDkOcwtC9tJGB23TD4DDZXLT7qgHuWyE5dG/G5KQl4i/v0uLtNSbGnuGTa9yfNbW/n5E1vpSWV43xIfb53v4f89GWdX1OJrZ5RwzPwa21LK+TFWF/kpDY8SEdbdaC/YucWaukYOP17TnOXLT8apjRjcdFZo2BD1ociYip+vSvL3LRmOrXTx/04JDGlBK6XY3GHx+M4Mj+/K0JZQ+N2wosbDmXUuTg7u4f43uvjTziJOWVjOfx4bHLZvT4DZJcG+749lQXPP6FMKKMOHZbhx5QzwzaUi4qcwMHobOJrI0tQ9OCilodvk+mcS7OiyiHiF754RYmFJ/8/C4zKoKw7mNTC2O5mlMbr/OgkJc9jcGgxnSgs3JhYGSgykd1yNIwbKEQclBhj7txkuN26X24m+G/s4uHBJBT3tzaSdxJvbt2/n1NPPoGHXDp546hm+94Mf89Nb76SqMIDfY3Dr//2Of/7jH/zpz3/ps2i+9MUv9OtvO+200wa5zj7xiU+wfPlyPvShD/W7vrZophvxYHkCWO4AljuI5Q5QW+bmpnqLO1/ezV9W7ua1va/xybMWcmxdMUoM7t7p55an49QVGNxwWpCqAW6E7tlnkiqYQ/WL36T26WvtyyiTdKiKjkWX011z2qABb4Pr5aLl6KuxvAWUbLqTEAnc5/w3Q+Y76Z0TBvC6DGYV+vtGIwOgFMVb/krpultJFc6j+bj/5vAdEYp6rCGlwTK8WO4AqvcH5YjW/LIQ5y+dxRE1Oaa6GGRDszB9RXhiezEyw+dMszwR0pE63B43pSEvxaGhJ3xzGcKc0iBbmnuGdKOdPL+MJdWF3PLUVm5d28rt61OICP994UKOnls5qD0a8Br5JdCMzLJFu3MnXtLUFgdHHFB5ZIWbr50a5KtPx7nuyTjfOSNIII/+qdaExY3PJljfavKuw7x8+CgfLsNFNliO6Yng7dnTl3dMRFhU4mJRiYuPHu1jbYvJ47syPLU7w6M7M/jdpSSzpVy22Mt/LPONGEBSGvb1a6T0WhoBz8BwawPLG8b0RLC8YZTLFuhsNok70Yor1UFvo6I05M1LZAAKAm4SGXf/nHjiZlZlJd9/VxH3rmnmpLklzC72kUl340p1YWS6AUVlxJd39oWI301Pyt0vus/+fQcgWM5YkyXl2+mfDy5DaGps4ZOf/iwf/4+Pg+HBcNnpd+pLQ4jAmjWv891vfYuf3fzLvuMMQwYFdXz5y1/m85//PPfeey+1tfZ4sERiwERy40RbNAOYiEWzpyNGe2zkr92mpm5++MgmdnckuGDpLETgwbWNnFBfwpdOLaIg2zrssUa6m9INf8Ry++ipOY1U4fwxZe/1uAxKQ14im+6GF34GNcvhvBvsxIyj0JuuX8w0Fat/QsHux+muOY2mYz49bFZjEDLhmhH99qPhSnbag0IHZB4wfSX4yuZQEvZR4HfnFUreEUvT0DHyD+e5ra387dU9vO/EOo6pGzpFz7zy0NgmLLNM6NzZlwwzlrZzS/Vze+Tw9O4M33guwdEV9oh07wjZjte2ZLnx2QTxrN3J/pY6D5angEy4qu+BPmJWb4eMqXilMcuTu7LMLjB47xLviJ9pyOumumj4AaVJfOyJu0kaIdtVOtL/x8rgTrRRanRTERrbdAmWBQ2dcZKWm2ygzP6ujZS6yDIp9yWZ5UnZ01fkmV08ayl2tcUxlcpjwObQCPZvcMypeQYO/BTB5fFx5JFHkslkcLncXPHe9/Gpz3wGwzB46sknefc7L+0Lby6vKOez13yet150EQDfuvEGCgsifOELXxh0qVtvvZWbbroJ0zQpKiriiCOO4Prrr6eqqn8Upw4GmCATEprOBO09oycUTGct/vDiTu5ZtQcFXHZsLR84aY6dDynRiie2d1zXHw6XCMUhL0WBnPlANv0Tnvyendjygu/YfQuj0Nq8l9CT1xPo2Ejr4e+nY9G7R+yPSRXMySuEdlQsE0+8EVeyDZcI4bLZFFbWjmvmw3z/R8NRFPQwu2Sc9xRrdSaAsyez2tuZIDWMK+3h7Wm+92KSk6rdfO3UwKBABaUU92/J8PNXk1SGDL5+aoD6kgCZUBWWb+j/pZgpPD17RrQS88ElwuzSYP9pLwy3PV2yr8B+dXkwLcXu9nhes3MWBT3MLvLbkwD2NOc/EZo7QCpQxpaoG0uN/gD3eQwWlIftnGOWZYtNshOS0VFzBva60MYjNIYIbpdgjOQm6xUUMZzfldEnLCNhKTXkdArDMZaBmcOhXWdvArxugw+vmMuK+WV0JzMsr9/f4jcDZSjDg7d7N8O1Pgcizjndhj13jNsleIz988e4DRn8XV10ge3WefR6uP/T8NbvQ2iEKQlaNlL68HWoZDd7T/h/xKpPGXZXyx0iXVCXV1aEYe9J7MScfo8Ln9uHr2whPmbjI4sEh7Y08qG60E8inSWRHnuqFsOAWYXjm/YZgFCZnc6/YwfubJLqIjvP3FB9R+fO9ZLIwk9WJvneCwm+lJPuJW0qfvRKkn9tz3BClZsvnxzEX1hBKlgxYmteuXykC+fhSnbYjZlxTjhbUeizRUYM8BdBsHTIqcJdhp0NuLk7SXM0NezUBBG/m9rigP1PD5XZS6LDntMoM3Q/Dt4IhCvAX4APqHVl2NUeH7HeIlBbHNg/XbFhONM6F9nu3FTUtjpT3fbcP0PUsyflJjHGdopL7N9hbn/leARlOOyknQZZa/TvtNswJiwy40ELzTSyeFZkyHLLV0jacOON7nRSt+8n6HUT9Lr6xMTjMsafVr7+VLjwe/DQdXDvJ+Bt/wOFQyT23Po4PPEdxF+IefFPSRs1YA79pTb9pWRC1Xn/aAzDbmH1iYrHwOc2hrFWxi9cvYgIdSUhtjT3jHkGycoC/5B9QGPCE4CyxRBtwB1vt8WmI0FmiM/z4oVe4lnFb15LEXAn+czxflriiq8/E2dzh8X7l3p53zFlmJEasq7BgQkuQ4j43YP6hEx/MaY3PHgW1jwoCngIhwpscQkUD0qxMxQVET9Br9t2PQ34zIM+F3UlQwQcBIrtJdVjj03qnYPHXwjhSluwcygMeihNe2kbwVotj/iGz0guYp+717LPJG3hSXXbi2MvlEV8tLaNest92I293uCAnOixScbjEkxL+jJBD4UhMuKkc1OJFppJxDvRh1AOlidEqnAe3ugOxEpjCJSF84vGGRPVx8BF/wsPfhHu+6QtPGUL7W3KgpX/B6/eBpVHwLk34A6WUJW1aOiI0/+ZIWTCtcNOPdBvT7FdJeUR37jcXxPF6zaoLQmws3XkFnAufo8xbCqYMWMY9kRvBbV4MjGqwz3sbmrBSsYGNSyuONxHPGOnrY9nFauaTDKW4vrTIyxfPIesr2jIS3jdBnNKg/g9LgLeFI1dyf4WheEhUzAHM9WFt2fvsNm3+xAX7lAJZXWzBz3k8yHsc7OwMsyu9jhxJ7VLb4e1MVIL2xe2l94Qe8/wFmVVoZ942iSRHmypBbxGXnnj+vD47SVcYbvY0rbgeJJRDBnNpLEtFo/bjcvlZkJzPuSJOCKSNkdIcDtEks4DxdR/AsMgIt8XkTdE5HURuVtEinK2fVlEtojIRhE5P6f8Aqdsi4hcm1M+V0RedMrvFBGvU+5z3m9xttdP5T2VR3zMLQ+NKZPqSCi3n1TRfPyBMHUlockXmV7KF8PFP7FdXfd/Bva9bk8d/PDXbZFZdCFc9AMI2i4+n9ugMteFJG7ShfNGFRkRKA55WFgZprY4OC0i00uB30NFQf4PnuqiwOT/SA0DfBG8RVXUzl9KtmIpqeLFZMJ1mP4ylDsICP9+pI93LPTyxC47Jc+P3l7HsUceiTWMyAS8LuaX75/Aqizso6506FBey1dIsngRpn/obArKHSQTriVVejhVcxYi4xCZXjwug3llIcoiXrxug/qyUP5unN4H/wjY1urg+7RdZsOHaY+KYdiWTmEtVC7BcHsRt9dJrdQbrm9ncFAuH8rtx+sL4PJ4D4jI9OIaYZK16XKZ9TJtwQAich7wmFIqKyLfBVBKfUlElgB/Ak4AqoFHgEXOYZuAc4EG4GXgPUqp9SLyZ+BvSqk7RORm4DWl1C9E5GrgKKXUx0XkCuDflFIjjjyaSDBAL0opWnvSNEWTw/ql80HEdteUh9zQscM25aeSnmb4x+ftsR+RKntK5hM/DkdePqQrrCOeoSUO6cL6/RFOQyDiZCIumB4LZiS2t8boGaWzekIBAGMgmTHZ1hLr715SCskmkHQPK3d3cVhdNYHg8A/7goCb2cXBIa2EZMZkR1uMTHboL6U9BXMDYmUxfcVk/SV9EYV5jRsaA5alRrZkJkA0melnrVYW+qiITKBvbQAbNmxg8WGHDdkBP3AOmQONaSlS2f4WnSG9k6dNXp3GGgwwbRaNUupfSvX5CV4AejsHLgHuUEqllFLbgS3YonMCsEUptU0plQbuAC4R+9M7C7jLOf5W4B0557rVWb8LOFsOgO0oIpRHfCyeFemb62GsBLwGCyrClEd8th+8ZJ49iHJMFTHA5c2/Uz5cARf/GErm2hFS538bjnrXsP0txUXFBKoPG1Zkel1kCyvDzsC+mSUyALOLR55/fcIBAGPA73ExrzzUvxEsgvIEsUIVHHPYwhFFpjTsZc4Irii/x8WC8jCBYUbcW54QqaKFJEsOJxOu7hOZiN89qSIDTJnIgG2tlkVsN2fA66J8kusOvf0d/R+fbmc2zOkSGaBvJs9cptNl1su0Cc0APgw86KzXALtztjU4ZcOVlwKdOaLVW97vXM72Lmf/fojIVSLyioi80tLSMik3BPY/eHZJcEzuNBGoKPAxvzzcf7Y7ESieY1savgJbdELl9vvC2fbMlaUL7PQnlUfArKPt1OqVS+0lUp1fJ6S/CC75Gbz3Dqg7ceT9ShdQWxoZ9OB6MwhML26X4XRGD729IjIJAQBjwO9xMa8sPGaPy6xCP9VFgyenG4jbcV8N2wDqjYbq21/siLA3GbMK/ISdaLapesi6cwZeelwGXrfrgD3Qv/nNb7J06VKOOuooli1bxosvvghANpultnoWX73u/9l1dFxmPT09/Md//Afz58/nuOOO44wzzug75kAwpcEAIvIIMNQkGNcppe519rkOyAJ/nMq6jIRS6hbgFrBdZ5N9/rDPzcKKMK09aZq7kwwXhejzGMwuDg7b4gTs0eZjRQQilXYYZ+duu2NzJHrHRAxHuNKeuwPbQz2nNMjWlh6yppqxLrKRCHrdzCr095vsCuz/R1l4kgIAxkDA66K+NMT21tiorlcRmF0cpHAMlrNhiNMASNIUHXm8Sm1xAPcBFNrJQkSYWzb+/qR88bgNXErhPoB9Mc8//zx///vfefXVV/H5fLS2tpJO2wEKDz/8MIsWLeJvf/srN37zW31RZh/96EeZO3cumzdvxjAMtm/fzvr16w9YnadUaJRS54y0XUQ+BFwEnK32dxbtAXKTddU6ZQxT3gYUiYjbsVpy9+89V4OIuIFCZ/8DTq87rSjoobErOSjktCzipTLin1KXAm4flC2AWJszcHCs4yjE7hAN9Z/EzOOyo4eAvOccn2mUhX0k0ma//8uUBADkScjnpr4sxI4RxKY3tc6YshTkUFHgx+s2aOhIDHmNskj/nHGawXz/5e/xRvsbk3rOw0oO40snfGnY7fv27aOsrAyfz3YJlpXt/z3+6U9/4tOf/jS/+MUvWPnSi5x66gq2bt3Kiy++yB//+EcMRxDnzp3L3LlzJ7XeIzGdUWcXAF8ELlZK5caZ3gdc4USMzQUWAi9hd/4vdCLMvMAVwH2OQD0OvNM5/krg3pxzXemsvxM7+GBaUyHkutP8HgOPW5hbHqKqMDC1IpNLqBQqDrfdX/kiht13M0BkevF7XG9akemlJmfu9cKAh/A4H+CTRdjnpq50aLee120wv2KMqXCGoCjoZV754OivgNcYOju1Zto577zz2L17N4sWLeLqq6/mySefBCCZTPLII4/w9re/nfe85z3ceecdAKxbt45ly5bhck3f73M6f0k/BXzAw06r8QWl1MeVUuucKLL12C61/1LKbnqLyCeAhwAX8FulVO+MQ18C7hCRbwCrgN845b8Bfi8iW4B2bHGaEYR9bhZUhFFqajtGh8XlsYUj0QldDWCNMI7C8NjBCN6pj7yaTgzDDo/d3ho7YAEAo1HgtyPedrfH+6wO27UWnDSXVtBrfxd3tsVIZqyJhwMfQoxkeUwV4XCYlStX8vTTT/P444/z7ne/m+985zuEw2HOPPNMAoEAl112GTfeeCM//OEPD3j9hkLnOhvAZIQ3v+mwTHt2yPgQCT3dAVtk3Ae+r2K6yJrWjOuX6Iyn2d2eGDF8eaL05iabiiizg4mhQnunk7vuuotbb70Vr9fLM888QyBgB280Nzdz7733Mm/ePM4991w2b948aVbNmya8WTODMFxQNBtKF0JuJmZfgZ0l4BASGWDGiQzsd3GNFL48UXpzk2mRmdls3LiRzZs3971fvXo15eXlPP300+zatYsdO3awY8cOfvazn/GnP/2J+fPns3z5cr72ta/Ra1js2LGDBx544IDVeeb9ojTThy9sh0eHZ0GwzLZk8shlpTkwTLQ/RnNw0NPTw5VXXsmSJUs46qijWL9+PW95y1s466yz+gIEAC655BLuv/9+UqkUv/71r2lqamLBggUcccQRfOhDH6KiouKA1Vm7zgZwSLrONBpN3sw019l0oF1nGo1Go5lRaKHRaDQazZSihUaj0WjGyKHc5TCee9dCo9FoNGPA7/fT1tZ2SIqNUoq2tjb8/rGNM9NhLBqNRjMGamtraWhoYDIT8L6Z8Pv91NYOMRPvCGih0Wg0mjHg8XgOaJ6wgwHtOtNoNBrNlKKFRqPRaDRTihYajUaj0UwpOjPAAESkBdjpvC0Dhsg0edCj7/vQ4lC870PxnmFq73uOUqp8qA1aaEZARF4ZLqXCwYy+70OLQ/G+D8V7hum7b+0602g0Gs2UooVGo9FoNFOKFpqRuWW6KzBN6Ps+tDgU7/tQvGeYpvvWfTQajUajmVK0RaPRaDSaKUULjUaj0WimFC00QyAiF4jIRhHZIiLXTnd9xoOI/FZEmkVkbU5ZiYg8LCKbnddip1xE5MfO/b4uIsfmHHOls/9mEbkyp/w4EVnjHPNjEZmaiezHiIjMFpHHRWS9iKwTkU875Qf1vYuIX0ReEpHXnPu+3imfKyIvOnW9U0S8TrnPeb/F2V6fc64vO+UbReT8nPIZ+bsQEZeIrBKRvzvvD4V73uF8B1eLyCtO2cz9jiul9JKzAC5gKzAP8AKvAUumu17juI/TgWOBtTll3wOuddavBb7rrL8VeBAQ4CTgRae8BNjmvBY768XOtpecfcU59sLpvmenXlXAsc56BNgELDnY792pS9hZ9wAvOnX8M3CFU34z8J/O+tXAzc76FcCdzvoS5zvvA+Y6vwXXTP5dANcAtwN/d94fCve8AygbUDZjv+PaohnMCcAWpdQ2pVQauAO4ZJrrNGaUUk8B7QOKLwFuddZvBd6RU36bsnkBKBKRKuB84GGlVLtSqgN4GLjA2VaglHpB2d/K23LONa0opfYppV511ruBDUANB/m9O/Xvcd56nEUBZwF3OeUD77v387gLONtptV4C3KGUSimltgNbsH8TM/J3ISK1wNuAXzvvhYP8nkdgxn7HtdAMpgbYnfO+wSk7GKhUSu1z1huBSmd9uHseqbxhiPIZheMaOQa7dX/Q37vjQloNNGM/NLYCnUqprLNLbl377s/Z3gWUMvbPY7r5IfBFwHLel3Lw3zPYjYh/ichKEbnKKZux33E9H80hilJKichBG9suImHgr8BnlFLRXBfzwXrvSikTWCYiRcDdwGHTW6OpRUQuApqVUitF5Ixprs6B5lSl1B4RqQAeFpE3cjfOtO+4tmgGsweYnfO+1ik7GGhyzGKc12anfLh7Hqm8dojyGYGIeLBF5o9Kqb85xYfEvQMopTqBx4GTsd0kvQ3K3Lr23Z+zvRBoY+yfx3SyArhYRHZgu7XOAn7EwX3PACil9jivzdiNihOYyd/x6e7UmmkLtpW3DbtTsLcDcOl012uc91JP/2CA79O/s/B7zvrb6N9Z+JJTXgJsx+4oLHbWS5xtAzsL3zrd9+vUS7B9yj8cUH5Q3ztQDhQ56wHgaeAi4C/07xi/2ln/L/p3jP/ZWV9K/47xbdid4jP6dwGcwf5ggIP6noEQEMlZfw64YCZ/x6f9CzITF+wojU3YPu7rprs+47yHPwH7gAy2j/Uj2P7oR4HNwCM5XyoBfubc7xpgec55PozdOboF+Pec8uXAWueYn+JkmZjuBTgV23/9OrDaWd56sN87cBSwyrnvtcBXnfJ5zkNjC/YD2OeU+533W5zt83LOdZ1zbxvJiTaayb8L+gvNQX3Pzv295izreus1k7/jOgWNRqPRaKYU3Uej0Wg0milFC41Go9FophQtNBqNRqOZUrTQaDQajWZK0UKj0Wg0milFC41GMwmISKmTSXe1iDSKyB5nvUdEfj6F1z1DRE6ZqvNrNJOBTkGj0UwCSqk2YBmAiHwd6FFK3XQALn0G0IM9aE+jmZFoi0ajmUIci6N3npSvi8itIvK0iOwUkUtF5HvOvB//dFLn9M4F8qSTMPGhnLQinxJ7np3XReQOJ2nox4HPOtbTaSJSLiJ/FZGXnWVFzrV/LyLPO3OPfMwprxKRp5zj14rIadPyQWkOarRFo9EcWOYDZ2LPgfI8cJlS6osicjfwNhF5APgJcIlSqkVE3g18E3sE97XAXKVUSkSKlFKdInIzOdaTiNwO/K9S6hkRqQMeAg53rn0UdlqRELDKudZ7gIeUUt8UERcQPDAfg+ZQQguNRnNgeVAplRGRNdj5tP7plK/Bzk23GDgCOyMvzj69qd9fB/4oIvcA9wxz/nOAJTnZqgucTNYA9yqlEkBCRB7HTsT4MvBbx5q6Rym1ehLuUaPphxYajebAkgJQSlkiklH7c0BZ2L9HAdYppU4e4ti3Yc+c+nbgOhE5coh9DOAkpVQyt9ARnoH5ppRS6ikROd059/+JyA+UUreN8940miHRfTQazcxiI1AuIieDPeWBiCwVEQOYrZR6HPgSdor7MNCNPWV1L/8CPtn7RkSW5Wy7RET8IlKKHUTwsojMAZqUUr/CnqXyWDSaSUYLjUYzg1D2lMHvBL4rIq9hZ58+BduF9gfH5bYK+LGy5525H/i33mAA4FPAcidgYD12sEAvr2PPU/MCcKNSai+24LwmIquAd2PP56LRTCo6e7NGcwhwgEOuNZp+aItGo9FoNFOKtmg0Go1GM6Voi0aj0Wg0U4oWGo1Go9FMKVpoNBqNRjOlaKHRaDQazZSihUaj0Wg0U8r/B3Y+tRCxSDGmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = {'PPO': './logs/dam/ppo',\n",
    "           'DDPG': './logs/dam/ddpg',\n",
    "           'SAC': './logs/dam/sac'}\n",
    "        \n",
    "plot_results(results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "01. Getting Started.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
